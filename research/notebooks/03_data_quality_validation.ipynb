{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNPL Data Quality Validation\n",
    "\n",
    "**Objective**: Comprehensive data quality assessment for production ML pipeline\n",
    "\n",
    "**Quality Dimensions**:\n",
    "1. **Completeness**: Missing values, null patterns\n",
    "2. **Consistency**: Data type consistency, format validation\n",
    "3. **Accuracy**: Range validation, business rule compliance\n",
    "4. **Uniqueness**: Duplicate detection, identifier integrity\n",
    "5. **Validity**: Schema compliance, constraint validation\n",
    "6. **Timeliness**: Data freshness, temporal coverage\n",
    "\n",
    "**Business Impact**: Poor data quality directly affects model performance and business decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# BigQuery integration\n",
    "from google.cloud import bigquery\n",
    "from flit_ml.config import config\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Data quality validation environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Profiling Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to BigQuery\n",
    "client = config.get_client()\n",
    "\n",
    "# Get comprehensive data profile\n",
    "profile_query = \"\"\"\n",
    "WITH data_profile AS (\n",
    "  SELECT\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(DISTINCT transaction_id) as unique_transactions,\n",
    "    MIN(transaction_date) as earliest_date,\n",
    "    MAX(transaction_date) as latest_date,\n",
    "    COUNT(DISTINCT DATE(transaction_date)) as active_days,\n",
    "    AVG(transaction_amount) as avg_amount,\n",
    "    STDDEV(transaction_amount) as std_amount,\n",
    "    MIN(transaction_amount) as min_amount,\n",
    "    MAX(transaction_amount) as max_amount\n",
    "  FROM `flit-data-platform.flit_staging.stg_bnpl_raw_transactions`\n",
    ")\n",
    "SELECT \n",
    "  *,\n",
    "  DATE_DIFF(latest_date, earliest_date, DAY) as date_range_days,\n",
    "  total_records / unique_customers as avg_transactions_per_customer,\n",
    "  total_records / active_days as avg_transactions_per_day\n",
    "FROM data_profile\n",
    "\"\"\"\n",
    "\n",
    "profile_df = client.query(profile_query).to_dataframe()\n",
    "print(\"üìä Data Profile Overview:\")\n",
    "profile_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Completeness Analysis\n",
    "\n",
    "Identify missing values and null patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing value statistics for all columns\n",
    "missing_analysis_query = \"\"\"\n",
    "SELECT\n",
    "  column_name,\n",
    "  data_type,\n",
    "  is_nullable,\n",
    "  -- Calculate missing percentages using conditional aggregation\n",
    "  ROUND(\n",
    "    COUNTIF(\n",
    "      CASE column_name\n",
    "        WHEN 'customer_id' THEN customer_id IS NULL\n",
    "        WHEN 'transaction_id' THEN transaction_id IS NULL\n",
    "        WHEN 'transaction_amount' THEN transaction_amount IS NULL\n",
    "        WHEN 'transaction_date' THEN transaction_date IS NULL\n",
    "        -- Add more columns as needed based on actual schema\n",
    "        ELSE FALSE\n",
    "      END\n",
    "    ) * 100.0 / COUNT(*), 2\n",
    "  ) as missing_percentage\n",
    "FROM \n",
    "  `flit-data-platform.flit_staging.INFORMATION_SCHEMA.COLUMNS` c\n",
    "CROSS JOIN \n",
    "  `flit-data-platform.flit_staging.stg_bnpl_raw_transactions` t\n",
    "WHERE \n",
    "  c.table_name = 'stg_bnpl_raw_transactions'\n",
    "GROUP BY column_name, data_type, is_nullable\n",
    "ORDER BY missing_percentage DESC\n",
    "\"\"\"\n",
    "\n",
    "# Note: This query template needs adjustment based on actual schema\n",
    "print(\"üìã Missing Value Analysis Template Created\")\n",
    "print(\"Will implement after schema exploration\")\n",
    "\n",
    "# Alternative approach: Sample data analysis\n",
    "sample_query = \"\"\"\n",
    "SELECT *\n",
    "FROM `flit-data-platform.flit_staging.stg_bnpl_raw_transactions`\n",
    "ORDER BY RAND()\n",
    "LIMIT 10000\n",
    "\"\"\"\n",
    "\n",
    "sample_df = client.query(sample_query).to_dataframe()\n",
    "print(f\"\\nüì• Loaded sample: {sample_df.shape}\")\n",
    "\n",
    "# Missing value analysis on sample\n",
    "missing_stats = {\n",
    "    'column': sample_df.columns,\n",
    "    'missing_count': sample_df.isnull().sum(),\n",
    "    'missing_percentage': (sample_df.isnull().sum() / len(sample_df) * 100).round(2),\n",
    "    'data_type': sample_df.dtypes\n",
    "}\n",
    "\n",
    "missing_df = pd.DataFrame(missing_stats)\n",
    "missing_df = missing_df[missing_df['missing_count'] > 0].sort_values('missing_percentage', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"\\n‚ùì Missing Values Found:\")\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values detected in sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consistency Validation\n",
    "\n",
    "Check data type consistency and format validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_consistency(df):\n",
    "    \"\"\"\n",
    "    Validate data consistency across different dimensions.\n",
    "    \"\"\"\n",
    "    consistency_report = {}\n",
    "    \n",
    "    # 1. Identifier consistency\n",
    "    if 'customer_id' in df.columns:\n",
    "        # Check for consistent customer ID format\n",
    "        customer_id_lengths = df['customer_id'].astype(str).str.len().value_counts()\n",
    "        consistency_report['customer_id_formats'] = customer_id_lengths.to_dict()\n",
    "    \n",
    "    # 2. Date format consistency\n",
    "    date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
    "    for col in date_columns:\n",
    "        try:\n",
    "            pd.to_datetime(df[col])\n",
    "            consistency_report[f'{col}_parseable'] = True\n",
    "        except:\n",
    "            consistency_report[f'{col}_parseable'] = False\n",
    "    \n",
    "    # 3. Numeric range validation\n",
    "    if 'transaction_amount' in df.columns:\n",
    "        amount_stats = {\n",
    "            'negative_amounts': (df['transaction_amount'] < 0).sum(),\n",
    "            'zero_amounts': (df['transaction_amount'] == 0).sum(),\n",
    "            'extreme_amounts': (df['transaction_amount'] > 10000).sum()  # Business threshold\n",
    "        }\n",
    "        consistency_report['transaction_amount_issues'] = amount_stats\n",
    "    \n",
    "    # 4. Text field consistency\n",
    "    text_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_columns:\n",
    "        if col not in ['customer_id', 'transaction_id']:  # Skip IDs\n",
    "            # Check for consistent casing\n",
    "            sample_values = df[col].dropna().head(100)\n",
    "            if len(sample_values) > 0:\n",
    "                mixed_case = sum(1 for val in sample_values if str(val) != str(val).lower() and str(val) != str(val).upper())\n",
    "                consistency_report[f'{col}_mixed_case_count'] = mixed_case\n",
    "    \n",
    "    return consistency_report\n",
    "\n",
    "# Run consistency validation\n",
    "print(\"üîç Running consistency validation...\")\n",
    "consistency_results = validate_data_consistency(sample_df)\n",
    "\n",
    "print(\"\\nüìä Consistency Validation Results:\")\n",
    "for key, value in consistency_results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Business Rule Validation\n",
    "\n",
    "Validate business logic and domain constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_business_rules(df):\n",
    "    \"\"\"\n",
    "    Validate BNPL-specific business rules.\n",
    "    \"\"\"\n",
    "    business_violations = []\n",
    "    \n",
    "    # Rule 1: Transaction amounts should be positive\n",
    "    if 'transaction_amount' in df.columns:\n",
    "        negative_amounts = (df['transaction_amount'] <= 0).sum()\n",
    "        if negative_amounts > 0:\n",
    "            business_violations.append(f\"Rule violation: {negative_amounts} non-positive transaction amounts\")\n",
    "    \n",
    "    # Rule 2: Future transaction dates should not exist\n",
    "    if 'transaction_date' in df.columns:\n",
    "        df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "        future_dates = (df['transaction_date'] > datetime.now()).sum()\n",
    "        if future_dates > 0:\n",
    "            business_violations.append(f\"Rule violation: {future_dates} future transaction dates\")\n",
    "    \n",
    "    # Rule 3: Customer age should be reasonable (18-100)\n",
    "    if 'customer_age' in df.columns:\n",
    "        invalid_ages = ((df['customer_age'] < 18) | (df['customer_age'] > 100)).sum()\n",
    "        if invalid_ages > 0:\n",
    "            business_violations.append(f\"Rule violation: {invalid_ages} invalid customer ages\")\n",
    "    \n",
    "    # Rule 4: Credit scores should be in valid range (300-850)\n",
    "    if 'credit_score' in df.columns:\n",
    "        invalid_scores = ((df['credit_score'] < 300) | (df['credit_score'] > 850)).sum()\n",
    "        if invalid_scores > 0:\n",
    "            business_violations.append(f\"Rule violation: {invalid_scores} invalid credit scores\")\n",
    "    \n",
    "    # Rule 5: Payment dates should not be before transaction dates\n",
    "    if 'payment_date' in df.columns and 'transaction_date' in df.columns:\n",
    "        df['payment_date'] = pd.to_datetime(df['payment_date'])\n",
    "        early_payments = (df['payment_date'] < df['transaction_date']).sum()\n",
    "        if early_payments > 0:\n",
    "            business_violations.append(f\"Rule violation: {early_payments} payments before transaction dates\")\n",
    "    \n",
    "    return business_violations\n",
    "\n",
    "# Run business rule validation\n",
    "print(\"‚öñÔ∏è  Validating business rules...\")\n",
    "violations = validate_business_rules(sample_df)\n",
    "\n",
    "if violations:\n",
    "    print(\"\\n‚ùå Business Rule Violations:\")\n",
    "    for violation in violations:\n",
    "        print(f\"  - {violation}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All business rules passed validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Duplicate Detection\n",
    "\n",
    "Identify duplicate records and data integrity issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_duplicates(df):\n",
    "    \"\"\"\n",
    "    Comprehensive duplicate detection analysis.\n",
    "    \"\"\"\n",
    "    duplicate_analysis = {}\n",
    "    \n",
    "    # 1. Complete row duplicates\n",
    "    complete_duplicates = df.duplicated().sum()\n",
    "    duplicate_analysis['complete_row_duplicates'] = complete_duplicates\n",
    "    \n",
    "    # 2. Transaction ID duplicates\n",
    "    if 'transaction_id' in df.columns:\n",
    "        transaction_duplicates = df['transaction_id'].duplicated().sum()\n",
    "        duplicate_analysis['transaction_id_duplicates'] = transaction_duplicates\n",
    "    \n",
    "    # 3. Customer + date + amount duplicates (potential same transaction)\n",
    "    if all(col in df.columns for col in ['customer_id', 'transaction_date', 'transaction_amount']):\n",
    "        business_duplicates = df.duplicated(subset=['customer_id', 'transaction_date', 'transaction_amount']).sum()\n",
    "        duplicate_analysis['potential_business_duplicates'] = business_duplicates\n",
    "    \n",
    "    # 4. Customer behavior duplicates (same customer, same amount, close dates)\n",
    "    if 'customer_id' in df.columns and 'transaction_amount' in df.columns:\n",
    "        # Group by customer and amount, check for close dates\n",
    "        suspicious_patterns = 0\n",
    "        for customer in df['customer_id'].unique()[:100]:  # Sample for performance\n",
    "            customer_data = df[df['customer_id'] == customer]\n",
    "            amount_counts = customer_data['transaction_amount'].value_counts()\n",
    "            # Flag customers with many identical amounts\n",
    "            if (amount_counts > 3).any():\n",
    "                suspicious_patterns += 1\n",
    "        \n",
    "        duplicate_analysis['customers_with_suspicious_patterns'] = suspicious_patterns\n",
    "    \n",
    "    return duplicate_analysis\n",
    "\n",
    "# Run duplicate detection\n",
    "print(\"üîç Detecting duplicates...\")\n",
    "duplicate_results = detect_duplicates(sample_df)\n",
    "\n",
    "print(\"\\nüìä Duplicate Detection Results:\")\n",
    "for key, value in duplicate_results.items():\n",
    "    status = \"‚ùå\" if value > 0 else \"‚úÖ\"\n",
    "    print(f\"{status} {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Outlier Detection\n",
    "\n",
    "Identify statistical anomalies that may indicate data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_statistical_outliers(df):\n",
    "    \"\"\"\n",
    "    Detect statistical outliers using multiple methods.\n",
    "    \"\"\"\n",
    "    outlier_analysis = {}\n",
    "    \n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns and df[col].notna().sum() > 0:\n",
    "            \n",
    "            # IQR method\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            iqr_outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "            \n",
    "            # Z-score method (|z| > 3)\n",
    "            z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "            z_outliers = (z_scores > 3).sum()\n",
    "            \n",
    "            outlier_analysis[col] = {\n",
    "                'iqr_outliers': iqr_outliers,\n",
    "                'z_score_outliers': z_outliers,\n",
    "                'outlier_percentage': round((iqr_outliers / len(df)) * 100, 2)\n",
    "            }\n",
    "    \n",
    "    return outlier_analysis\n",
    "\n",
    "# Run outlier detection\n",
    "print(\"üìà Detecting statistical outliers...\")\n",
    "outlier_results = detect_statistical_outliers(sample_df)\n",
    "\n",
    "print(\"\\nüìä Statistical Outlier Results:\")\n",
    "for column, stats in outlier_results.items():\n",
    "    if stats['iqr_outliers'] > 0 or stats['z_score_outliers'] > 0:\n",
    "        print(f\"\\n{column}:\")\n",
    "        print(f\"  IQR outliers: {stats['iqr_outliers']} ({stats['outlier_percentage']}%)\")\n",
    "        print(f\"  Z-score outliers: {stats['z_score_outliers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Scorecard\n",
    "\n",
    "Comprehensive data quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_quality_scorecard(df, consistency_results, violations, duplicate_results, outlier_results):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality scorecard.\n",
    "    \"\"\"\n",
    "    scorecard = {}\n",
    "    \n",
    "    # 1. Completeness Score (0-100)\n",
    "    missing_percentage = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    completeness_score = max(0, 100 - missing_percentage * 2)  # Penalize missing values\n",
    "    scorecard['completeness_score'] = round(completeness_score, 1)\n",
    "    \n",
    "    # 2. Consistency Score (0-100)\n",
    "    consistency_issues = sum(1 for key, value in consistency_results.items() \n",
    "                           if 'parseable' in key and value == False)\n",
    "    consistency_score = max(0, 100 - consistency_issues * 10)\n",
    "    scorecard['consistency_score'] = round(consistency_score, 1)\n",
    "    \n",
    "    # 3. Validity Score (0-100)\n",
    "    validity_score = max(0, 100 - len(violations) * 5)\n",
    "    scorecard['validity_score'] = round(validity_score, 1)\n",
    "    \n",
    "    # 4. Uniqueness Score (0-100)\n",
    "    duplicate_percentage = (duplicate_results.get('complete_row_duplicates', 0) / len(df)) * 100\n",
    "    uniqueness_score = max(0, 100 - duplicate_percentage * 5)\n",
    "    scorecard['uniqueness_score'] = round(uniqueness_score, 1)\n",
    "    \n",
    "    # 5. Accuracy Score (outlier-based, 0-100)\n",
    "    total_outliers = sum(stats['iqr_outliers'] for stats in outlier_results.values())\n",
    "    outlier_percentage = (total_outliers / len(df)) * 100\n",
    "    accuracy_score = max(0, 100 - outlier_percentage)\n",
    "    scorecard['accuracy_score'] = round(accuracy_score, 1)\n",
    "    \n",
    "    # 6. Overall Quality Score (weighted average)\n",
    "    weights = {\n",
    "        'completeness_score': 0.25,\n",
    "        'consistency_score': 0.20,\n",
    "        'validity_score': 0.25,\n",
    "        'uniqueness_score': 0.15,\n",
    "        'accuracy_score': 0.15\n",
    "    }\n",
    "    \n",
    "    overall_score = sum(scorecard[metric] * weight for metric, weight in weights.items())\n",
    "    scorecard['overall_quality_score'] = round(overall_score, 1)\n",
    "    \n",
    "    return scorecard\n",
    "\n",
    "# Generate scorecard\n",
    "scorecard = generate_data_quality_scorecard(\n",
    "    sample_df, consistency_results, violations, duplicate_results, outlier_results\n",
    ")\n",
    "\n",
    "print(\"\\nüèÜ Data Quality Scorecard:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, score in scorecard.items():\n",
    "    if score >= 80:\n",
    "        status = \"üü¢ Excellent\"\n",
    "    elif score >= 60:\n",
    "        status = \"üü° Good\"\n",
    "    elif score >= 40:\n",
    "        status = \"üü† Fair\"\n",
    "    else:\n",
    "        status = \"üî¥ Poor\"\n",
    "    \n",
    "    print(f\"{metric.replace('_', ' ').title()}: {score}/100 {status}\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Recommendations\n",
    "\n",
    "Actionable recommendations for data quality improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(scorecard, consistency_results, violations, duplicate_results):\n",
    "    \"\"\"\n",
    "    Generate actionable data quality recommendations.\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Completeness recommendations\n",
    "    if scorecard['completeness_score'] < 80:\n",
    "        recommendations.append({\n",
    "            'priority': 'High',\n",
    "            'category': 'Completeness',\n",
    "            'issue': 'Significant missing values detected',\n",
    "            'recommendation': 'Implement data validation at source, add required field constraints'\n",
    "        })\n",
    "    \n",
    "    # Validity recommendations\n",
    "    if violations:\n",
    "        recommendations.append({\n",
    "            'priority': 'High',\n",
    "            'category': 'Validity',\n",
    "            'issue': f'{len(violations)} business rule violations found',\n",
    "            'recommendation': 'Add business rule validation in data pipeline, implement data contracts'\n",
    "        })\n",
    "    \n",
    "    # Uniqueness recommendations\n",
    "    if duplicate_results.get('complete_row_duplicates', 0) > 0:\n",
    "        recommendations.append({\n",
    "            'priority': 'Medium',\n",
    "            'category': 'Uniqueness',\n",
    "            'issue': 'Duplicate records detected',\n",
    "            'recommendation': 'Implement deduplication logic, add unique constraints on key fields'\n",
    "        })\n",
    "    \n",
    "    # Consistency recommendations\n",
    "    consistency_issues = [k for k, v in consistency_results.items() if 'parseable' in k and v == False]\n",
    "    if consistency_issues:\n",
    "        recommendations.append({\n",
    "            'priority': 'Medium',\n",
    "            'category': 'Consistency',\n",
    "            'issue': 'Data format inconsistencies found',\n",
    "            'recommendation': 'Standardize data formats, implement schema validation'\n",
    "        })\n",
    "    \n",
    "    # Overall quality recommendations\n",
    "    if scorecard['overall_quality_score'] < 70:\n",
    "        recommendations.append({\n",
    "            'priority': 'High',\n",
    "            'category': 'Overall',\n",
    "            'issue': 'Overall data quality below acceptable threshold',\n",
    "            'recommendation': 'Implement comprehensive data quality monitoring, establish data governance processes'\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_recommendations(scorecard, consistency_results, violations, duplicate_results)\n",
    "\n",
    "print(\"\\nüìã Data Quality Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if recommendations:\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        priority_emoji = \"üî¥\" if rec['priority'] == 'High' else \"üü°\" if rec['priority'] == 'Medium' else \"üü¢\"\n",
    "        print(f\"\\n{i}. {priority_emoji} {rec['priority']} Priority - {rec['category']}\")\n",
    "        print(f\"   Issue: {rec['issue']}\")\n",
    "        print(f\"   Recommendation: {rec['recommendation']}\")\nelse:\n",
    "    print(\"\\n‚úÖ No immediate data quality issues requiring attention.\")\n",
    "    print(\"   Continue monitoring data quality metrics regularly.\")\n",
    "\n",
    "print(\"\\n=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "**Data Quality Assessment Complete**\n",
    "\n",
    "This analysis provides a comprehensive view of data quality across multiple dimensions. Use these insights to:\n",
    "\n",
    "1. **Prioritize data quality improvements** based on business impact\n",
    "2. **Implement monitoring** for ongoing quality assurance\n",
    "3. **Design feature engineering** with quality constraints in mind\n",
    "4. **Set model performance expectations** based on data quality\n",
    "\n",
    "**Next Actions**:\n",
    "- Address high-priority data quality issues\n",
    "- Proceed with feature engineering on validated data\n",
    "- Implement automated data quality monitoring\n",
    "- Document data quality requirements for production pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.11.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}