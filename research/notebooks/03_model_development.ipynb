{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNPL ML Model Development\n",
    "\n",
    "**Objective**: Develop production-ready ML model for BNPL default risk prediction\n",
    "\n",
    "**Performance Targets**:\n",
    "- Beat current 3.5x risk discrimination baseline\n",
    "- Achieve >40% precision on high-risk segment\n",
    "- Maintain <100ms inference latency\n",
    "- Production deployment ready\n",
    "\n",
    "**Focus**: Primary target is `will_default` prediction (binary classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Steps\n",
    "\n",
    "### **Step 1: Algorithm Research & Selection** \n",
    "- Evaluate ML algorithms for BNPL risk assessment\n",
    "- Consider inference latency requirements (<100ms)\n",
    "- Assess interpretability for regulatory compliance\n",
    "- Select candidate algorithms for testing\n",
    "\n",
    "### **Step 2: Data Loading & Feature Engineering**\n",
    "- Load engineered features using `BNPLFeatureEngineer` class\n",
    "- Validate feature quality and distribution\n",
    "- Prepare data for modeling\n",
    "\n",
    "### **Step 3: Baseline Performance Establishment**\n",
    "- Implement current underwriting baseline (3.5x discrimination)\n",
    "- Train candidate models on engineered features\n",
    "- Establish performance benchmarks\n",
    "\n",
    "### **Step 4: Model Training & Evaluation**\n",
    "- Cross-validation framework\n",
    "- Business metrics: discrimination ratio, precision/recall\n",
    "- Technical metrics: latency, model size, memory usage\n",
    "- Feature importance analysis\n",
    "\n",
    "### **Step 5: Production Readiness Assessment**\n",
    "- Inference latency benchmarking\n",
    "- Model serialization and deployment format\n",
    "- Edge case handling and fallback strategies\n",
    "- A/B testing framework preparation\n",
    "\n",
    "### **Step 6: Model Selection & Recommendations**\n",
    "- Compare algorithms across business and technical dimensions\n",
    "- Select final model for production deployment\n",
    "- Document trade-offs and deployment considerations\n",
    "- Prepare ML engineering handoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path for imports\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "print(f\"Project root added to path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (ExtraTreesClassifier, HistGradientBoostingClassifier,\n",
    "                            VotingClassifier, StackingClassifier, AdaBoostClassifier,\n",
    "                            GradientBoostingClassifier, BaggingClassifier)\n",
    "from sklearn.naive_bayes import ComplementNB, BernoulliNB\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Import models for training\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# For feature importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom feature engineering class\n",
    "from flit_ml.features.bnpl_feature_engineering import BNPLFeatureEngineer\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"ML development environment ready!\")\n",
    "print(f\"Available ML libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Algorithm Research & Selection\n",
    "\n",
    "### ML Algorithm Comparison for BNPL Risk Assessment\n",
    "\n",
    "| Algorithm | Inference Speed | Interpretability | Performance | Memory Usage | Production Ready | Pros | Cons |\n",
    "|-----------|----------------|------------------|-------------|--------------|------------------|------|------|\n",
    "| **LogisticRegression** | Fastest (~1ms) | High (coefficients) | Good baseline | Minimal | Excellent | Fast inference, Interpretable, Stable, Low memory | Linear assumptions, May miss complex patterns |\n",
    "| **RandomForestClassifier** | Fast (~5-10ms) | Medium (feature importance) | Strong for tabular | Moderate | Good | Handles non-linearity, Feature importance, Robust | Larger model size, Less interpretable |\n",
    "| **XGBoost** | Fast (~10-20ms) | Medium (SHAP values) | Often best for tabular | Moderate | Good | High performance, Feature importance, Handles missing values | Hyperparameter tuning, Model complexity |\n",
    "| **LightGBM** | Very Fast (~5ms) | Medium (SHAP values) | Excellent for tabular | Low | Excellent | Fast training/inference, Memory efficient, High performance | Can overfit small datasets, Less interpretable than linear |\n",
    "\n",
    "### Selection Criteria for BNPL Production System\n",
    "\n",
    "1. **Inference latency**: <100ms (preferably <20ms)\n",
    "2. **Performance**: Beat 3.5x discrimination ratio\n",
    "3. **Interpretability**: Regulatory compliance requirements\n",
    "4. **Production stability**: Minimal maintenance overhead\n",
    "5. **Memory efficiency**: Scalable serving architecture\n",
    "\n",
    "### Recommended Starting Models\n",
    "\n",
    "Based on BNPL constraints, we'll focus on all 4 models before considering more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading & Feature Engineering\n",
    "\n",
    "Load engineered features using our production-ready `BNPLFeatureEngineer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer with clean output for development                                                                                                                                       \n",
    "feature_engineer = BNPLFeatureEngineer(verbose=True)                                                                                                                                               \n",
    "                                                                                                                                                                                                        \n",
    "# Load and engineer features                                                                                                                                                                          \n",
    "print(\"🚀 Loading and engineering features for model development...\")                                                                                                                                 \n",
    "df_features, feature_metadata = feature_engineer.engineer_features(                                                                                                                                   \n",
    "    sample_size=1000,  # Sample size for development                                                                                                                                                  \n",
    "    random_seed=42                                                                                                                                                                                    \n",
    ")                                                                                                                                                                                                     \n",
    "                                                                                                                                                                                                      \n",
    "print(f\"\\n📊 Feature Engineering Complete:\")                                                                                                                                                          \n",
    "print(f\"   Dataset shape: {df_features.shape}\")                                                                                                                                                       \n",
    "print(f\"   Features available: {len(feature_metadata['all_features'])}\")                                                                                                                              \n",
    "print(f\"   Target variable: {feature_metadata['target_variable']}\")                                                                                                                                   \n",
    "print(f\"   Default rate: {df_features['will_default'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate feature quality and distribution\n",
    "print(\"🔍 Feature Quality Assessment:\")\n",
    "print(f\"   Data shape: {df_features.shape}\")\n",
    "print(f\"   Memory usage: {df_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"   Default rate: {df_features['will_default'].mean():.1%}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "missing_values = df_features.isnull().sum().sum()\n",
    "print(f\"   Missing values: {missing_values}\")\n",
    "\n",
    "# Feature type summary\n",
    "print(f\"\\n📋 Feature Types:\")\n",
    "for feature_type, features in feature_metadata.items():\n",
    "    if isinstance(features, list) and feature_type.endswith('_features'):\n",
    "        print(f\"   {feature_type}: {len(features)} features\")\n",
    "\n",
    "# Display sample of features\n",
    "print(f\"\\n📋 Sample of engineered features:\")\n",
    "display_cols = df_features.columns[:10].tolist()\n",
    "if 'will_default' not in display_cols:\n",
    "    display_cols.append('will_default')\n",
    "    \n",
    "df_features[display_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"🔧 Preparing data for model training...\")\n",
    "\n",
    "# Separate features and target - exclude both target variables\n",
    "exclude_cols = ['will_default', 'days_to_first_missed_payment']  # Remove both target variables, even though the 2nd one is secondary (for more enhanced models)\n",
    "feature_cols = [col for col in df_features.columns if col not in exclude_cols]\n",
    "numeric_features = ['amount', 'risk_score', 'payment_credit_limit', 'price_comparison_time', 'customer_tenure_days']\n",
    "categorical_features = [col for col in feature_cols if col not in numeric_features]\n",
    "primary_target_col = exclude_cols[0]  # 'will_default'\n",
    "secondary_target_col = exclude_cols[1]  # 'days_to_first_missed_payment'\n",
    "\n",
    "# For now, we will focus on the primary target\n",
    "target_col = primary_target_col\n",
    "\n",
    "X = df_features[feature_cols]\n",
    "y = df_features[target_col]\n",
    "\n",
    "print(f\"   Features shape: {X.shape}\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "print(f\"   Target distribution: {y.value_counts().to_dict()}\")\n",
    "print(f\"   Class balance: {y.mean():.1%} positive class\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Train-Test Split:\")\n",
    "print(f\"   Training set: {X_train.shape}\")\n",
    "print(f\"   Test set: {X_test.shape}\")\n",
    "print(f\"   Train default rate: {y_train.mean():.1%}\")\n",
    "print(f\"   Test default rate: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling for algorithms that need it\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    " # Create preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_features),      # Scale numeric features\n",
    "    ('cat', 'passthrough', categorical_features)      # Leave categorical unchanged\n",
    "])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)  # Fit AND transform train\n",
    "X_test_processed = preprocessor.transform(X_test)        # Only transform test (using train's fit) --> Avoid inconsistent scaling train vs test\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Data preparation complete\")\n",
    "print(f\"   Ready for model training and evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Baseline Performance Establishment\n",
    "\n",
    "Implement current underwriting baseline and train candidate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current underwriting baseline analysis\n",
    "print(\"📊 Current Underwriting Baseline Analysis\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Analyze current risk_score and risk_level performance\n",
    "if 'risk_score' in df_features.columns and 'risk_level_encoded' in df_features.columns:\n",
    "    \n",
    "    # Risk level performance\n",
    "    risk_performance = df_features.groupby('risk_level_encoded')['will_default'].agg(['count', 'mean']).round(3)\n",
    "    risk_performance.columns = ['transaction_count', 'default_rate']\n",
    "    \n",
    "    print(f\"\\n🎯 Current Risk Level Performance:\")\n",
    "    risk_level_mapping = {0: 'Low', 1: 'Medium', 2: 'High'}\n",
    "    for idx, row in risk_performance.iterrows():\n",
    "        level_name = risk_level_mapping.get(idx, f'Level_{idx}')\n",
    "        print(f\"   {level_name} Risk: {row['default_rate']:.1%} default rate ({row['transaction_count']:,} transactions)\")\n",
    "    \n",
    "    # Calculate discrimination ratio\n",
    "    high_risk_rate = risk_performance.loc[2, 'default_rate']  # High risk (encoded as 2)\n",
    "    low_risk_rate = risk_performance.loc[0, 'default_rate']   # Low risk (encoded as 0)\n",
    "    current_discrimination = high_risk_rate / low_risk_rate if low_risk_rate > 0 else 0\n",
    "    \n",
    "    print(f\"\\n📈 Current System Discrimination:\")\n",
    "    print(f\"   High risk default rate: {high_risk_rate:.1%}\")\n",
    "    print(f\"   Low risk default rate: {low_risk_rate:.1%}\")\n",
    "    print(f\"   Discrimination ratio: {current_discrimination:.1f}x\")\n",
    "    print(f\"   Target to beat: >{current_discrimination:.1f}x\")\n",
    "    \n",
    "    # Risk score distribution analysis\n",
    "    print(f\"\\n📊 Risk Score Distribution:\")\n",
    "    risk_score_stats = df_features['risk_score'].describe()\n",
    "    print(f\"   Range: {risk_score_stats['min']:.2f} - {risk_score_stats['max']:.2f}\")\n",
    "    print(f\"   Mean: {risk_score_stats['mean']:.2f}\")\n",
    "    print(f\"   Std: {risk_score_stats['std']:.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Risk score/level features not available in dataset\")\n",
    "    current_discrimination = 3.5  # Use known baseline from context\n",
    "    print(f\"   Using known baseline discrimination ratio: {current_discrimination}x\")\n",
    "\n",
    "# Set performance targets\n",
    "target_discrimination = max(current_discrimination * 1.1, 4.0)  # At least 10% improvement or 4.0x\n",
    "target_precision = 0.40  # 40% precision on high-risk segment\n",
    "\n",
    "print(f\"\\n🎯 Performance Targets for ML Models:\")\n",
    "print(f\"   Discrimination ratio: >{target_discrimination:.1f}x\")\n",
    "print(f\"   High-risk precision: >{target_precision:.0%}\")\n",
    "print(f\"   Inference latency: <100ms\")\n",
    "print(f\"   Model size: <50MB (for fast loading)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training & Cross-Validation\n",
    "\n",
    "Train candidate models with proper preprocessing and evaluate using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models for comparison\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbosity=-1),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "print(f\"🤖 Models initialized: {list(models.keys())}\")\n",
    "\n",
    "# Cross-validation setup\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(f\"📊 Cross-validation: {cv_folds.n_splits}-fold stratified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = {}\n",
    "\n",
    "print(\"🚀 Starting model training and cross-validation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n🔧 Training {model_name}...\")\n",
    "\n",
    "    # Use appropriate data (scaled for LR, unscaled for tree-based)\n",
    "    if model_name == 'LogisticRegression':\n",
    "        X_train_model = X_train_processed\n",
    "        X_test_model = X_test_processed\n",
    "    else:\n",
    "        # Tree-based models don't need scaling\n",
    "        X_train_model = X_train.values\n",
    "        X_test_model = X_test.values\n",
    "\n",
    "    # Cross-validation\n",
    "    start_time = time.time()\n",
    "    cv_scores = cross_val_score(model, X_train_model, y_train, cv=cv_folds, scoring='roc_auc')\n",
    "    cv_time = time.time() - start_time\n",
    "\n",
    "    # Train on full training set\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_model, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Store results\n",
    "    cv_results[model_name] = {\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'cv_time': cv_time,\n",
    "        'train_time': train_time,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "    print(f\"   CV ROC-AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    print(f\"   CV Time: {cv_time:.2f}s, Train Time: {train_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n✅ Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial model training gives AUC of ~0.5, showing poor performance across models. Let's investigate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations with target variable\n",
    "print(\"🔍 Feature-Target Correlations:\")\n",
    "target_corrs = df_features.corr()['will_default'].drop(['will_default','days_to_first_missed_payment']).sort_values(key=abs, ascending=False)\n",
    "print(target_corrs.head(10))\n",
    "\n",
    "# Check if we have any strong correlations\n",
    "strong_corrs = target_corrs[abs(target_corrs) > 0.1]\n",
    "print(f\"\\nFeatures with |correlation| > 0.1: {len(strong_corrs)}\")\n",
    "if len(strong_corrs) > 0:\n",
    "    print(strong_corrs)\n",
    "else:\n",
    "    print(\"⚠️ No features have strong correlation with target!\")\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Focus on features most correlated with target\n",
    "top_features = target_corrs.head(15).index.tolist() + ['will_default']\n",
    "corr_matrix = df_features[top_features].corr()\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0,\n",
    "            fmt='.2f', square=True, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Feature Correlation Heatmap (Top 15 vs Target)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quick sanity checks\n",
    "print(f\"\\n📊 Data Sanity Checks:\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Any missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Feature columns: {list(X.columns[:5])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train columns:\", list(X_train.columns))\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "\n",
    "\n",
    "# Correlations for features actually used in training\n",
    "train_data_with_target = pd.concat([X_train, y_train], axis=1)\n",
    "actual_corrs = train_data_with_target.corr()['will_default'].drop('will_default').sort_values(key=abs, ascending=False)\n",
    "print(\"Actual training feature correlations:\")\n",
    "print(actual_corrs.head(10))\n",
    "\n",
    "\n",
    "print(\"Are we using X_train for tree models?\", type(X_train))\n",
    "print(\"Are we using X_train_processed for LogReg?\", type(X_train_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_target = pd.concat([X_train, y_train], axis=1)\n",
    "actual_corrs = train_data_with_target.corr()['will_default'].drop('will_default').sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"🔍 ACTUAL Training Feature Correlations:\")\n",
    "print(actual_corrs.head(10))\n",
    "\n",
    "print(f\"\\nFeatures with |correlation| > 0.05:\")\n",
    "strong_corrs = actual_corrs[abs(actual_corrs) > 0.05]\n",
    "print(strong_corrs)\n",
    "\n",
    "print(f\"\\nX_train columns: {list(X_train.columns)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if current risk_level shows expected discrimination\n",
    "print(\"Risk level performance check:\")\n",
    "risk_perf = df_features.groupby('risk_level_encoded')['will_default'].mean()\n",
    "print(risk_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features removed during cleaning:\")\n",
    "removed_features = feature_metadata.get('features_removed', [])\n",
    "for feature in removed_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "print(f\"\\nTotal removed: {len(removed_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 4 models gave a performance as poor as random, and yet there seems to be no obvious reason apart from the poor correlation btn features and the predicted. So this is mostly a data quality issue --  our data is not predictive or it's just plain unrealistic (again, feedback for Simtom). Question is, though, it is possible to still get some predictive power from features that have such low correlation levels to our target variable? To determine this, we will brute-force our eval of models and just throw many models at the problem to see which one likely gives something reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete model portfolio - All Tiers\n",
    "models = {\n",
    "    # Original models\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbosity=-1),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "\n",
    "    # Tier 1: Must Try\n",
    "    'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42),\n",
    "    'ExtraTrees': ExtraTreesClassifier(random_state=42, n_estimators=100),\n",
    "    #'ComplementNB': ComplementNB(),\n",
    "\n",
    "    # Tier 2: High Value\n",
    "    'Ridge': RidgeClassifier(random_state=42),\n",
    "    'ElasticNet': LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga',\n",
    "                                    random_state=42, max_iter=1000),\n",
    "    #'SVM_Linear': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    #'MLPClassifier': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500),\n",
    "\n",
    "    # Tier 3: Experimental  \n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'KNN_Weighted': KNeighborsClassifier(n_neighbors=10, weights='distance'),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'BernoulliNB': BernoulliNB(),\n",
    "    'SGDClassifier': SGDClassifier(random_state=42, loss='log_loss', max_iter=1000),\n",
    "    'BaggingClassifier': BaggingClassifier(random_state=42),\n",
    "    'SVM_RBF': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "}\n",
    "\n",
    "print(f\"🤖 Complete model portfolio: {len(models)} algorithms\")\n",
    "print(\"Tier breakdown:\")\n",
    "print(\"  Original: LogisticRegression, RandomForest, LightGBM, XGBoost\")\n",
    "print(\"  Tier 1: HistGradientBoosting, ExtraTrees, ComplementNB\")\n",
    "print(\"  Tier 2: Ridge, ElasticNet, SVM_Linear, MLPClassifier\")\n",
    "print(\"  Tier 3: KNN variants, AdaBoost, GradientBoosting, BernoulliNB, SGD, Bagging, SVM_RBF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training with proper error handling and model-specific logic\n",
    "cv_results = {}\n",
    "\n",
    "# Model-specific configurations\n",
    "model_configs = {\n",
    "    'SVM_RBF': {'max_samples': 10000},  # Subsample for speed\n",
    "    'SVM_Linear': {'max_samples': 20000},\n",
    "    'KNN': {'max_samples': 15000},\n",
    "    'KNN_Weighted': {'max_samples': 15000},\n",
    "}\n",
    "\n",
    "print(\"🚀 Starting robust model training and evaluation...\")\n",
    "print(f\"Training {len(models)} models on {X_train.shape[0]:,} samples\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n🔧 Training {model_name}...\")\n",
    "\n",
    "    # Determine preprocessing needs\n",
    "    scaling_models = ['LogisticRegression', 'ElasticNet', 'SVM_Linear', 'SVM_RBF',\n",
    "                    'MLPClassifier', 'KNN', 'KNN_Weighted', 'SGDClassifier','Ridge']\n",
    "\n",
    "    # Prepare data\n",
    "    if model_name in scaling_models:\n",
    "        X_train_model = X_train_processed\n",
    "        X_test_model = X_test_processed\n",
    "        data_type = \"scaled\"\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "        data_type = \"unscaled\"\n",
    "\n",
    "    # Apply model-specific sampling if needed\n",
    "    if model_name in model_configs:\n",
    "        max_samples = model_configs[model_name].get('max_samples')\n",
    "        if max_samples and len(X_train_model) > max_samples:\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            X_sample, _, y_sample, _ = train_test_split(\n",
    "                X_train_model, y_train, train_size=max_samples,\n",
    "                stratify=y_train, random_state=42\n",
    "            )\n",
    "            X_train_model, y_train_model = X_sample, y_sample\n",
    "            print(f\"   Using subsample: {len(X_train_model):,} samples\")\n",
    "        else:\n",
    "            y_train_model = y_train\n",
    "    else:\n",
    "        y_train_model = y_train\n",
    "\n",
    "    try:\n",
    "        # Cross-validation with timeout\n",
    "        start_time = time.time()\n",
    "        cv_scores = cross_val_score(model, X_train_model, y_train_model,\n",
    "                                    cv=cv_folds, scoring='roc_auc', n_jobs=1)\n",
    "        cv_time = time.time() - start_time\n",
    "\n",
    "        # Skip if CV takes too long\n",
    "        if cv_time > 300:  # 5 minutes timeout\n",
    "            print(f\"   ⏰ Timeout: CV took {cv_time:.1f}s, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Train on full training set\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train_model, y_train_model)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Test predictions with proper handling\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test_model)[:, 1]\n",
    "        elif hasattr(model, 'decision_function'):\n",
    "            # For Ridge, SVM without probability\n",
    "            y_pred_proba = model.decision_function(X_test_model)\n",
    "        else:\n",
    "            print(f\"   ⚠️ No probability/decision function available\")\n",
    "            continue\n",
    "\n",
    "        test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        # Store results\n",
    "        cv_results[model_name] = {\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'test_auc': test_auc,\n",
    "            'cv_time': cv_time,\n",
    "            'train_time': train_time,\n",
    "            'model': model,\n",
    "            'data_type': data_type,\n",
    "            'status': 'success',\n",
    "            'samples_used': len(X_train_model)\n",
    "        }\n",
    "\n",
    "        print(f\"   CV ROC-AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "        print(f\"   Test ROC-AUC: {test_auc:.3f}\")\n",
    "        print(f\"   Times: CV={cv_time:.1f}s, Train={train_time:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Failed: {str(e)[:100]}\")\n",
    "        cv_results[model_name] = {'status': 'failed', 'error': str(e)}\n",
    "\n",
    "print(f\"\\n✅ Robust training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract successful results\n",
    "successful_models = {k: v for k, v in cv_results.items() if v.get('status') == 'success'}\n",
    "\n",
    "if len(successful_models) == 0:\n",
    "    print(\"❌ No successful models to plot!\")\n",
    "else:\n",
    "    # Create subplots for multiple comparisons\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Performance Comparison Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. ROC-AUC Performance Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    model_names = list(successful_models.keys())\n",
    "    test_aucs = [successful_models[name]['test_auc'] for name in model_names]\n",
    "    cv_means = [successful_models[name]['cv_mean'] for name in model_names]\n",
    "    cv_stds = [successful_models[name]['cv_std'] for name in model_names]\n",
    "\n",
    "    # Sort by test performance\n",
    "    sorted_indices = sorted(range(len(test_aucs)), key=lambda i: test_aucs[i], reverse=True)\n",
    "    model_names_sorted = [model_names[i] for i in sorted_indices]\n",
    "    test_aucs_sorted = [test_aucs[i] for i in sorted_indices]\n",
    "    cv_means_sorted = [cv_means[i] for i in sorted_indices]\n",
    "    cv_stds_sorted = [cv_stds[i] for i in sorted_indices]\n",
    "\n",
    "    x_pos = range(len(model_names_sorted))\n",
    "    bars1 = ax1.bar(x_pos, test_aucs_sorted, alpha=0.7, label='Test ROC-AUC', color='steelblue')\n",
    "    bars2 = ax1.bar([x + 0.4 for x in x_pos], cv_means_sorted, alpha=0.7,\n",
    "                    yerr=cv_stds_sorted, label='CV ROC-AUC (±std)', color='orange', width=0.4)\n",
    "\n",
    "    ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random (0.5)')\n",
    "    ax1.set_xlabel('Models (sorted by Test performance)')\n",
    "    ax1.set_ylabel('ROC-AUC Score')\n",
    "    ax1.set_title('Model ROC-AUC Performance Comparison')\n",
    "    ax1.set_xticks([x + 0.2 for x in x_pos])\n",
    "    ax1.set_xticklabels(model_names_sorted, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "        ax1.text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.01,\n",
    "                f'{test_aucs_sorted[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # 2. Training Time vs Performance\n",
    "    ax2 = axes[0, 1]\n",
    "    train_times = [successful_models[name]['train_time'] for name in model_names]\n",
    "    colors = ['red' if auc < 0.55 else 'orange' if auc < 0.65 else 'green'\n",
    "            for auc in test_aucs]\n",
    "\n",
    "    scatter = ax2.scatter(train_times, test_aucs, c=colors, s=100, alpha=0.7)\n",
    "    for i, name in enumerate(model_names):\n",
    "        ax2.annotate(name, (train_times[i], test_aucs[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "    ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.7)\n",
    "    ax2.set_xlabel('Training Time (seconds)')\n",
    "    ax2.set_ylabel('Test ROC-AUC')\n",
    "    ax2.set_title('Performance vs Training Speed')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Cross-Validation Consistency (CV mean vs std)\n",
    "    ax3 = axes[1, 0]\n",
    "    cv_consistency = ax3.scatter(cv_means, cv_stds, s=100, alpha=0.7, c=test_aucs,\n",
    "                                cmap='RdYlGn', vmin=0.4, vmax=0.8)\n",
    "    for i, name in enumerate(model_names):\n",
    "        ax3.annotate(name, (cv_means[i], cv_stds[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "    ax3.axvline(x=0.5, color='red', linestyle='--', alpha=0.7)\n",
    "    ax3.set_xlabel('CV Mean ROC-AUC')\n",
    "    ax3.set_ylabel('CV Standard Deviation')\n",
    "    ax3.set_title('Model Consistency (Lower std = more stable)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    plt.colorbar(cv_consistency, ax=ax3, label='Test ROC-AUC')\n",
    "\n",
    "    # 4. Top Models Detailed Comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    # Show top 6 models\n",
    "    top_n = min(6, len(model_names_sorted))\n",
    "    top_models = model_names_sorted[:top_n]\n",
    "    top_test_aucs = test_aucs_sorted[:top_n]\n",
    "    top_cv_means = cv_means_sorted[:top_n]\n",
    "    top_cv_stds = cv_stds_sorted[:top_n]\n",
    "\n",
    "    x_pos_top = range(top_n)\n",
    "    ax4.barh(x_pos_top, top_test_aucs, alpha=0.7, color='steelblue', label='Test ROC-AUC')\n",
    "    ax4.errorbar(top_cv_means, x_pos_top, xerr=top_cv_stds, fmt='o',\n",
    "                color='orange', label='CV ROC-AUC (±std)')\n",
    "\n",
    "    ax4.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Random')\n",
    "    ax4.set_yticks(x_pos_top)\n",
    "    ax4.set_yticklabels(top_models)\n",
    "    ax4.set_xlabel('ROC-AUC Score')\n",
    "    ax4.set_title(f'Top {top_n} Models - Detailed View')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, auc in enumerate(top_test_aucs):\n",
    "        ax4.text(auc + 0.01, i, f'{auc:.3f}', va='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary statistics table\n",
    "print(f\"\\n📊 Performance Summary Table\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<20} {'Test AUC':<10} {'CV Mean':<10} {'CV Std':<10} {'Train Time':<12}\")\n",
    "print(\"-\"*80)\n",
    "for name in model_names_sorted:\n",
    "    results = successful_models[name]\n",
    "    print(f\"{name:<20} {results['test_auc']:<10.3f} {results['cv_mean']:<10.3f} \"\n",
    "        f\"{results['cv_std']:<10.3f} {results['train_time']:<12.2f}s\")\n",
    "\n",
    "# Quick insights\n",
    "best_model = model_names_sorted[0]\n",
    "print(f\"\\n🏆 Best performing model: {best_model} (AUC: {successful_models[best_model]['test_auc']:.3f})\")\n",
    "\n",
    "breakthrough_models = [name for name, results in successful_models.items()\n",
    "                    if results['test_auc'] > 0.55]\n",
    "if breakthrough_models:\n",
    "    print(f\"🚀 Models breaking through 0.55 barrier: {len(breakthrough_models)}\")\n",
    "    for name in breakthrough_models:\n",
    "        print(f\"   - {name}: {successful_models[name]['test_auc']:.3f}\")\n",
    "else:\n",
    "    print(\"⚠️  No models achieved AUC > 0.55\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select Top Models for Tuning\n",
    "# Extract top 5 models by performance with speed considerations\n",
    "top_models_for_tuning = []\n",
    "for name in model_names_sorted[:8]:  # Consider top 8\n",
    "    results = successful_models[name]\n",
    "    # Balance: AUC > 0.6 AND training time < 5s (adjustable threshold)\n",
    "    if results['test_auc'] >= 0.59 and results['train_time'] < 5.0:\n",
    "        top_models_for_tuning.append((name, results))\n",
    "\n",
    "print(\"🎯 Selected models for hyperparameter tuning:\")\n",
    "for name, results in top_models_for_tuning[:10]:\n",
    "    print(f\"  {name}: AUC={results['test_auc']:.3f}, Time={results['train_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids focusing on speed AND performance\n",
    "param_grids = {\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [50, 100],  # Reduce from default for speed\n",
    "        'learning_rate': [0.1, 0.15, 0.2],  # Higher LR = fewer iterations\n",
    "        'max_depth': [3, 5],  # Shallower trees = faster\n",
    "        'subsample': [0.8, 1.0],  # Subsampling for speed\n",
    "    },\n",
    "\n",
    "    'Ridge': {\n",
    "        'alpha': [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr'],\n",
    "        'fit_intercept': [True, False]\n",
    "    },\n",
    "\n",
    "    'ElasticNet': {\n",
    "        'C': [0.1, 1.0, 10.0, 100.0],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "        'max_iter': [500, 1000],  # Reduce iterations for speed\n",
    "        'tol': [1e-3, 1e-4]  # Relaxed tolerance for speed\n",
    "    },\n",
    "\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'max_iter': [1000, 2000],\n",
    "        'tol': [1e-3, 1e-4]\n",
    "    },\n",
    "\n",
    "    'BernoulliNB': {\n",
    "        'alpha': [0.01, 0.1, 0.5, 1.0, 2.0],\n",
    "        'fit_prior': [True, False],\n",
    "        'binarize': [0.0, 0.1, 0.5]\n",
    "    },\n",
    "\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],  # Test if more helps\n",
    "        'learning_rate': [0.5, 1.0, 1.5],\n",
    "        'algorithm': ['SAMME', 'SAMME.R']\n",
    "    },\n",
    "\n",
    "    'HistGradientBoosting': {\n",
    "        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
    "        'max_iter': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 10, None],\n",
    "        'min_samples_leaf': [5, 10, 20]\n",
    "    },\n",
    "\n",
    "    'SGDClassifier': {\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "        'eta0': [0.01, 0.1, 1.0],\n",
    "        'max_iter': [500, 1000],\n",
    "        'tol': [1e-3, 1e-4]\n",
    "    },\n",
    "\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'num_leaves': [20, 31, 50],\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'min_child_samples': [10, 20],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🎯 Tuning all 9 models with AUC ≥ 0.6\")\n",
    "print(\"Focus areas:\")\n",
    "print(\"  Speed optimization: GradientBoosting, ElasticNet\")\n",
    "print(\"  Performance boost: Ridge, LogisticRegression, BernoulliNB, HistGB, SGD\")\n",
    "print(\"  Push over threshold: LightGBM (0.599 → 0.6+)\")\n",
    "print(\"  Balance optimization: AdaBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Efficient Tuning Strategy\n",
    "\n",
    "tuned_models = {}\n",
    "\n",
    "print(\"🔧 Starting hyperparameter tuning for top models...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, _ in top_models_for_tuning:\n",
    "    if model_name not in param_grids:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🎯 Tuning {model_name}...\")\n",
    "\n",
    "    # Get base model and data\n",
    "    base_model = models[model_name]\n",
    "    if model_name in scaling_models:\n",
    "        X_tune, y_tune = X_train_processed, y_train\n",
    "    else:\n",
    "        X_tune, y_tune = X_train.values, y_train\n",
    "\n",
    "    try:\n",
    "        # Use RandomizedSearchCV for speed\n",
    "        search = RandomizedSearchCV(\n",
    "            base_model,\n",
    "            param_grids[model_name],\n",
    "            n_iter=25,  # Increased for better coverage\n",
    "            cv=3,       # 3-fold for speed\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        search.fit(X_tune, y_tune)\n",
    "        tune_time = time.time() - start_time\n",
    "\n",
    "        # Test best model with proper probability handling\n",
    "        best_model = search.best_estimator_\n",
    "        if model_name in scaling_models:\n",
    "            X_test_model = X_test_processed\n",
    "        else:\n",
    "            X_test_model = X_test.values\n",
    "\n",
    "        # Handle different model prediction methods\n",
    "        if hasattr(best_model, 'predict_proba'):\n",
    "            y_pred_proba = best_model.predict_proba(X_test_model)[:, 1]\n",
    "        elif hasattr(best_model, 'decision_function'):\n",
    "            # For Ridge, SVM without probability\n",
    "            y_pred_proba = best_model.decision_function(X_test_model)\n",
    "        else:\n",
    "            print(f\"   ⚠️  No probability method available for {model_name}\")\n",
    "            continue\n",
    "\n",
    "        tuned_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        baseline_auc = successful_models[model_name]['test_auc']\n",
    "        improvement = tuned_auc - baseline_auc\n",
    "\n",
    "        tuned_models[model_name] = {\n",
    "            'model': best_model,\n",
    "            'best_params': search.best_params_,\n",
    "            'best_cv_score': search.best_score_,\n",
    "            'test_auc': tuned_auc,\n",
    "            'baseline_auc': baseline_auc,\n",
    "            'improvement': improvement,\n",
    "            'tune_time': tune_time,\n",
    "            'status': 'success'\n",
    "        }\n",
    "\n",
    "        print(f\"   Best params: {search.best_params_}\")\n",
    "        print(f\"   Baseline AUC: {baseline_auc:.3f} → Tuned AUC: {tuned_auc:.3f}\")\n",
    "        print(f\"   Improvement: {improvement:+.3f} ({tune_time:.1f}s tuning)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Tuning failed: {str(e)[:100]}\")\n",
    "        tuned_models[model_name] = {'status': 'failed', 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of tuning results\n",
    "successful_tuning = {k: v for k, v in tuned_models.items() if v.get('status') == 'success'}\n",
    "failed_tuning = {k: v for k, v in tuned_models.items() if v.get('status') == 'failed'}\n",
    "\n",
    "print(f\"\\n📈 Hyperparameter Tuning Results:\")\n",
    "print(\"=\"*60)\n",
    "if successful_tuning:\n",
    "    for name, results in sorted(successful_tuning.items(), key=lambda x: x[1]['test_auc'], reverse=True):\n",
    "        print(f\"{name:20} {results['baseline_auc']:.3f} → {results['test_auc']:.3f} \"\n",
    "            f\"({results['improvement']:+.3f}) [{results['tune_time']:.1f}s]\")\n",
    "\n",
    "    # Best tuned model\n",
    "    best_tuned = max(successful_tuning.items(), key=lambda x: x[1]['test_auc'])\n",
    "    print(f\"\\n🏆 Best tuned model: {best_tuned[0]} (AUC: {best_tuned[1]['test_auc']:.3f})\")\n",
    "\n",
    "    # Biggest improvement\n",
    "    best_improvement = max(successful_tuning.items(), key=lambda x: x[1]['improvement'])\n",
    "    print(f\"📈 Biggest improvement: {best_improvement[0]} ({best_improvement[1]['improvement']:+.3f})\")\n",
    "\n",
    "if failed_tuning:\n",
    "    print(f\"\\n❌ Failed tuning: {list(failed_tuning.keys())}\")\n",
    "\n",
    "# Compare with original baseline\n",
    "print(f\"\\n🆚 Best Models Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(\"ORIGINAL BEST:\")\n",
    "orig_best = model_names_sorted[0]\n",
    "print(f\"  {orig_best}: {successful_models[orig_best]['test_auc']:.3f}\")\n",
    "\n",
    "if successful_tuning:\n",
    "    tuned_best = best_tuned[0]\n",
    "    print(\"AFTER TUNING:\")\n",
    "    print(f\"  {tuned_best}: {best_tuned[1]['test_auc']:.3f}\")\n",
    "    overall_improvement = best_tuned[1]['test_auc'] - successful_models[orig_best]['test_auc']\n",
    "    print(f\"OVERALL IMPROVEMENT: {overall_improvement:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model against Business Usefulness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having evaluated the top models and determined that Ridge is best, we will proceed to do a business assessment to see if this model has any business value. If there is value, however little, we will ship to prod and then later improve the quality of the model and the quality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_business_metrics(model, X_test_data, y_test, model_name=\"Best Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate model using BNPL-specific business metrics\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model to evaluate\n",
    "        X_test_data: Test features (ensure correct scaling for model type)\n",
    "        y_test: Test targets\n",
    "        model_name: Name for display\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"📊 Business Metrics Evaluation: {model_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Generate predictions\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_data)[:, 1]\n",
    "        pred_type = \"predict_proba\"\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        y_pred_proba = model.decision_function(X_test_data)\n",
    "        pred_type = \"decision_function\"\n",
    "    else:\n",
    "        raise ValueError(\"Model has no probability/decision function\")\n",
    "\n",
    "    print(f\"Prediction method: {pred_type}\")\n",
    "    print(f\"Data shape: {X_test_data.shape}\")\n",
    "\n",
    "    # 1. Overall ROC-AUC\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"ROC-AUC Score: {auc_score:.3f}\")\n",
    "\n",
    "    # 2. Risk-based segmentation (equal-sized tertiles)\n",
    "    try:\n",
    "        risk_segments_equal = pd.qcut(y_pred_proba, q=3, labels=['Low', 'Medium', 'High'])\n",
    "        segment_perf_equal = pd.DataFrame({\n",
    "            'segment': risk_segments_equal,\n",
    "            'actual_default': y_test.values\n",
    "        }).groupby('segment')['actual_default'].agg(['count', 'mean']).round(3)\n",
    "\n",
    "        discrimination_equal = segment_perf_equal.loc['High', 'mean'] / segment_perf_equal.loc['Low', 'mean']\n",
    "\n",
    "        print(f\"\\n1️⃣ Equal-Sized Risk Segments:\")\n",
    "        print(segment_perf_equal)\n",
    "        print(f\"   Discrimination Ratio: {discrimination_equal:.1f}x\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n1️⃣ Equal-Sized Risk Segments: Error - {e}\")\n",
    "        discrimination_equal = 0.0\n",
    "\n",
    "    # 3. Top 10% highest risk (more business-relevant)\n",
    "    try:\n",
    "        top_10_threshold = np.percentile(y_pred_proba, 90)\n",
    "        top_10_mask = y_pred_proba >= top_10_threshold\n",
    "\n",
    "        top_10_precision = y_test.values[top_10_mask].mean()\n",
    "        bottom_90_rate = y_test.values[~top_10_mask].mean()\n",
    "        discrimination_top10 = top_10_precision / bottom_90_rate if bottom_90_rate > 0 else float('inf')\n",
    "\n",
    "        print(f\"\\n2️⃣ Top 10% Highest Risk Analysis:\")\n",
    "        print(f\"   Top 10% default rate: {top_10_precision:.1%}\")\n",
    "        print(f\"   Bottom 90% default rate: {bottom_90_rate:.1%}\")\n",
    "        print(f\"   Discrimination Ratio: {discrimination_top10:.1f}x\")\n",
    "        print(f\"   Top 10% precision: {top_10_precision:.1%}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n2️⃣ Top 10% Analysis: Error - {e}\")\n",
    "        discrimination_top10 = 0.0\n",
    "        top_10_precision = 0.0\n",
    "\n",
    "    # 4. Compare to baseline system performance\n",
    "    print(f\"\\n3️⃣ vs Baseline System:\")\n",
    "    print(f\"   Current system discrimination: 2.8x\")\n",
    "    print(f\"   ML model discrimination (top 10%): {discrimination_top10:.1f}x\")\n",
    "    improvement = discrimination_top10 - 2.8\n",
    "    print(f\"   Improvement: {improvement:+.1f}x ({improvement/2.8*100:+.1f}%)\")\n",
    "\n",
    "    # 5. Business impact assessment\n",
    "    if discrimination_top10 > 4.0:\n",
    "        status = \"🎯 EXCEEDS TARGET (>4.0x)\"\n",
    "    elif discrimination_top10 > 2.8:\n",
    "        status = \"✅ BEATS BASELINE (>2.8x)\"\n",
    "    else:\n",
    "        status = \"❌ BELOW BASELINE (<2.8x)\"\n",
    "\n",
    "    print(f\"\\n4️⃣ Business Assessment: {status}\")\n",
    "\n",
    "    # Return key metrics\n",
    "    return {\n",
    "        'auc': auc_score,\n",
    "        'discrimination_equal': discrimination_equal,\n",
    "        'discrimination_top10': discrimination_top10,\n",
    "        'top10_precision': top_10_precision,\n",
    "        'improvement_vs_baseline': improvement,\n",
    "        'beats_baseline': discrimination_top10 > 2.8,\n",
    "        'exceeds_target': discrimination_top10 > 4.0\n",
    "    }\n",
    "\n",
    "# Helper function to use correct data for each model type\n",
    "def evaluate_model_with_correct_data(model_name, model_dict_key='tuned'):\n",
    "    \"\"\"Evaluate model with appropriate data scaling\"\"\"\n",
    "\n",
    "    if model_dict_key == 'tuned' and model_name in tuned_models:\n",
    "        model = tuned_models[model_name]['model']\n",
    "        source = \"tuned\"\n",
    "    elif model_name in successful_models:\n",
    "        model = successful_models[model_name]['model']\n",
    "        source = \"original\"\n",
    "    else:\n",
    "        print(f\"❌ Model {model_name} not found\")\n",
    "        return None\n",
    "\n",
    "    # Determine correct data type\n",
    "    scaling_models = ['Ridge', 'LogisticRegression', 'ElasticNet', 'SVM_Linear', 'SVM_RBF',\n",
    "                    'MLPClassifier', 'KNN', 'KNN_Weighted', 'SGDClassifier']\n",
    "\n",
    "    if model_name in scaling_models:\n",
    "        X_eval = X_test_processed\n",
    "        data_info = \"scaled\"\n",
    "    else:\n",
    "        X_eval = X_test.values\n",
    "        data_info = \"unscaled\"\n",
    "\n",
    "    print(f\"🔧 Evaluating {model_name} ({source} model) with {data_info} data\")\n",
    "\n",
    "    return evaluate_business_metrics(model, X_eval, y_test, f\"{model_name} ({source})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge with correct scaling\n",
    "ridge_metrics = evaluate_model_with_correct_data('Ridge', 'tuned')\n",
    "ridge_metrics = evaluate_model_with_correct_data('Ridge', 'tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuned_models['Ridge']['model']  # or whatever your best model is\n",
    "business_metrics = evaluate_business_metrics(best_model, X_test.values, y_test, \"Ridge Tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Ensemble Methods for BNPL Default Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "import time\n",
    "\n",
    "print(\"🎯 Advanced Ensemble Methods Training (Fast Version)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast individual models for ensembles (replaced HistGB with SGD)\n",
    "best_individual_models = [\n",
    "    ('ridge', RidgeClassifier(alpha=10.0, solver='cholesky', random_state=42)),\n",
    "    ('lgb', LGBMClassifier(random_state=42, verbosity=-1)),\n",
    "    ('xgb', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "    ('sgd', SGDClassifier(random_state=42, loss='log_loss', max_iter=1000)),\n",
    "    ('ada', AdaBoostClassifier(random_state=42))\n",
    "]\n",
    "# Create ensemble models\n",
    "ensemble_models = {}\n",
    "\n",
    "print(\"🔧 Creating fast ensemble models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Voting Classifier (Soft Voting)\n",
    "print(\"\\n1️⃣ Training Voting Classifier (Soft)...\")\n",
    "# Only use models with predict_proba for soft voting\n",
    "voting_models = [\n",
    "    ('lgb', LGBMClassifier(random_state=42, verbosity=-1)),\n",
    "    ('xgb', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "    ('sgd', SGDClassifier(random_state=42, loss='log_loss', max_iter=1000)),\n",
    "    ('ada', AdaBoostClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=voting_models,\n",
    "    voting='soft',  # Average predicted probabilities\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "voting_soft.fit(X_train.values, y_train)\n",
    "voting_soft_time = time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "y_pred_voting = voting_soft.predict_proba(X_test.values)[:, 1]\n",
    "voting_soft_auc = roc_auc_score(y_test, y_pred_voting)\n",
    "\n",
    "ensemble_models['VotingSoft'] = {\n",
    "    'model': voting_soft,\n",
    "    'auc': voting_soft_auc,\n",
    "    'train_time': voting_soft_time,\n",
    "    'data_type': 'unscaled'\n",
    "}\n",
    "\n",
    "print(f\"   Voting Soft AUC: {voting_soft_auc:.3f} (Time: {voting_soft_time:.1f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Voting Classifier (Hard Voting)\n",
    "print(\"\\n2️⃣ Training Voting Classifier (Hard)...\")\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=best_individual_models,\n",
    "    voting='hard',  # Majority vote\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "voting_hard.fit(X_train.values, y_train)\n",
    "voting_hard_time = time.time() - start_time\n",
    "\n",
    "# For AUC calculation, get individual predictions and average\n",
    "individual_preds = []\n",
    "for name, model in voting_models:\n",
    "    model.fit(X_train.values, y_train)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        pred = model.predict_proba(X_test.values)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        pred = model.decision_function(X_test.values)\n",
    "    individual_preds.append(pred)\n",
    "\n",
    "y_pred_voting_hard = np.mean(individual_preds, axis=0)\n",
    "voting_hard_auc = roc_auc_score(y_test, y_pred_voting_hard)\n",
    "\n",
    "ensemble_models['VotingHard'] = {\n",
    "    'model': voting_hard,\n",
    "    'auc': voting_hard_auc,\n",
    "    'train_time': voting_hard_time,\n",
    "    'data_type': 'unscaled'\n",
    "}\n",
    "\n",
    "print(f\"   Voting Hard AUC: {voting_hard_auc:.3f} (Time: {voting_hard_time:.1f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Stacking Classifier (Fast Version)\n",
    "print(\"\\n3️⃣ Training Stacking Classifier (Fast)...\")\n",
    "stacking_base_models = [\n",
    "    ('lgb', LGBMClassifier(random_state=42, verbosity=-1, n_estimators=50)),  # Reduced trees\n",
    "    ('xgb', XGBClassifier(random_state=42, eval_metric='logloss', n_estimators=50)),\n",
    "    ('sgd', SGDClassifier(random_state=42, loss='log_loss', max_iter=500)),  # Fast SGD\n",
    "    ('ada', AdaBoostClassifier(random_state=42, n_estimators=50))\n",
    "]\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=stacking_base_models,\n",
    "    final_estimator=LogisticRegression(random_state=42, max_iter=500),  # Fast meta-learner\n",
    "    cv=3,  # 3-fold CV for speed\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "stacking_clf.fit(X_train.values, y_train)\n",
    "stacking_time = time.time() - start_time\n",
    "\n",
    "y_pred_stacking = stacking_clf.predict_proba(X_test.values)[:, 1]\n",
    "stacking_auc = roc_auc_score(y_test, y_pred_stacking)\n",
    "\n",
    "ensemble_models['Stacking'] = {\n",
    "    'model': stacking_clf,\n",
    "    'auc': stacking_auc,\n",
    "    'train_time': stacking_time,\n",
    "    'data_type': 'unscaled'\n",
    "}\n",
    "\n",
    "print(f\"   Stacking AUC: {stacking_auc:.3f} (Time: {stacking_time:.1f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Bagging with LightGBM (Fast)\n",
    "print(\"\\n4️⃣ Training Bagging LightGBM (Fast)...\")\n",
    "\n",
    "bagging_lgb = BaggingClassifier(\n",
    "    estimator=LGBMClassifier(random_state=42, verbosity=-1, n_estimators=30),  # Smaller trees\n",
    "    n_estimators=5,  # Fewer bags for speed\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "bagging_lgb.fit(X_train.values, y_train)\n",
    "bagging_lgb_time = time.time() - start_time\n",
    "\n",
    "y_pred_bagging_lgb = bagging_lgb.predict_proba(X_test.values)[:, 1]\n",
    "bagging_lgb_auc = roc_auc_score(y_test, y_pred_bagging_lgb)\n",
    "\n",
    "ensemble_models['BaggingLGB'] = {\n",
    "    'model': bagging_lgb,\n",
    "    'auc': bagging_lgb_auc,\n",
    "    'train_time': bagging_lgb_time,\n",
    "    'data_type': 'unscaled'\n",
    "}\n",
    "\n",
    "print(f\"   Bagging LGB AUC: {bagging_lgb_auc:.3f} (Time: {bagging_lgb_time:.1f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Weighted Ensemble (Custom - Very Fast)\n",
    "print(\"\\n5️⃣ Training Weighted Ensemble...\")\n",
    "\n",
    "# Fast individual models for weighting\n",
    "individual_models = {\n",
    "    'lgb': LGBMClassifier(random_state=42, verbosity=-1, n_estimators=100),\n",
    "    'xgb': XGBClassifier(random_state=42, eval_metric='logloss', n_estimators=100),\n",
    "    'sgd': SGDClassifier(random_state=42, loss='log_loss', max_iter=1000),\n",
    "    'ada': AdaBoostClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "model_weights = {}\n",
    "individual_predictions = {}\n",
    "\n",
    "start_time = time.time()\n",
    "for name, model in individual_models.items():\n",
    "    # Quick CV score for weighting (3-fold for speed)\n",
    "    cv_scores = cross_val_score(model, X_train.values, y_train, cv=3, scoring='roc_auc')\n",
    "    model_weights[name] = cv_scores.mean()\n",
    "\n",
    "    # Train and predict\n",
    "    model.fit(X_train.values, y_train)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        individual_predictions[name] = model.predict_proba(X_test.values)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        individual_predictions[name] = model.decision_function(X_test.values)\n",
    "\n",
    "weighted_time = time.time() - start_time\n",
    "\n",
    "# Normalize weights\n",
    "total_weight = sum(model_weights.values())\n",
    "model_weights = {k: v/total_weight for k, v in model_weights.items()}\n",
    "\n",
    "# Weighted average prediction\n",
    "y_pred_weighted = sum(model_weights[name] * pred\n",
    "                    for name, pred in individual_predictions.items())\n",
    "\n",
    "weighted_auc = roc_auc_score(y_test, y_pred_weighted)\n",
    "\n",
    "ensemble_models['WeightedEnsemble'] = {\n",
    "    'model': None,  # Custom ensemble\n",
    "    'auc': weighted_auc,\n",
    "    'train_time': weighted_time,\n",
    "    'weights': model_weights,\n",
    "    'data_type': 'unscaled'\n",
    "}\n",
    "\n",
    "print(f\"   Weighted Ensemble AUC: {weighted_auc:.3f} (Time: {weighted_time:.1f}s)\")\n",
    "print(f\"   Model weights: {model_weights}\")\n",
    "\n",
    "# 6. Bonus: Super Fast Ensemble (SGD + LightGBM only)\n",
    "print(\"\\n6️⃣ Training Super Fast Ensemble (SGD + LGB)...\")\n",
    "\n",
    "super_fast_models = [\n",
    "    ('sgd', SGDClassifier(random_state=42, loss='log_loss', max_iter=500)),\n",
    "    ('lgb', LGBMClassifier(random_state=42, verbosity=-1, n_estimators=50))\n",
    "]\n",
    "\n",
    "super_fast_voting = VotingClassifier(\n",
    "    estimators=super_fast_models,\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "super_fast_voting.fit(X_train.values, y_train)\n",
    "super_fast_time = time.time() - start_time\n",
    "\n",
    "y_pred_super_fast = super_fast_voting.predict_proba(X_test.values)[:, 1]\n",
    "super_fast_auc = roc_auc_score(y_test, y_pred_super_fast)\n",
    "\n",
    "ensemble_models['SuperFast'] = {\n",
    "    'model': super_fast_voting,\n",
    "    'auc': super_fast_auc,\n",
    "    'train_time': super_fast_time,\n",
    "    'data_type': 'unscaled'\n",
    "}\n",
    "\n",
    "print(f\"   Super Fast AUC: {super_fast_auc:.3f} (Time: {super_fast_time:.1f}s)\")\n",
    "\n",
    "# Results Summary\n",
    "print(f\"\\n📊 Fast Ensemble Methods Results:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'AUC':<8} {'Train Time':<12} {'vs Best Individual':<15} {'Speed Grade'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "best_individual_auc = 0.618  # From your results\n",
    "for name, results in sorted(ensemble_models.items(), key=lambda x: x[1]['auc'], reverse=True):\n",
    "    improvement = results['auc'] - best_individual_auc\n",
    "    train_time_str = f\"{results['train_time']:.1f}s\" if results['train_time'] > 0 else \"Instant\"\n",
    "\n",
    "    # Speed grading\n",
    "    if results['train_time'] < 5:\n",
    "        speed_grade = \"🚀 Excellent\"\n",
    "    elif results['train_time'] < 15:\n",
    "        speed_grade = \"⚡ Good\"\n",
    "    else:\n",
    "        speed_grade = \"🐌 Slow\"\n",
    "\n",
    "    print(f\"{name:<20} {results['auc']:<8.3f} {train_time_str:<12} {improvement:+.3f} {'':10} {speed_grade}\")\n",
    "\n",
    "# Best ensemble\n",
    "best_ensemble = max(ensemble_models.items(), key=lambda x: x[1]['auc'])\n",
    "print(f\"\\n🏆 Best Fast Ensemble: {best_ensemble[0]} (AUC: {best_ensemble[1]['auc']:.3f})\")\n",
    "\n",
    "# Business metrics for best ensemble\n",
    "if best_ensemble[1]['model'] is not None:\n",
    "    print(f\"\\n🎯 Business Impact - Best Fast Ensemble ({best_ensemble[0]}):\")\n",
    "    ensemble_business_metrics = evaluate_business_metrics(\n",
    "        best_ensemble[1]['model'],\n",
    "        X_test.values,\n",
    "        y_test,\n",
    "        f\"{best_ensemble[0]} Ensemble\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n⚡ Speed Summary:\")\n",
    "print(f\"   Fastest: SuperFast (~{ensemble_models['SuperFast']['train_time']:.1f}s)\")\n",
    "print(f\"   Best Performance: {best_ensemble[0]} ({best_ensemble[1]['auc']:.3f} AUC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after trying these ensemble methods, Ridge still comes out on top. This is our prod model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Latency Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding latency performance for production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our final Ridge model\n",
    "ridge_model = tuned_models['Ridge']['model']  # Or use your best Ridge model\n",
    "scaler = preprocessor  # The ColumnTransformer we used\n",
    "\n",
    "print(f\"🎯 Model: Ridge Classifier (Production Candidate)\")\n",
    "print(f\"   Parameters: {ridge_model.get_params()}\")\n",
    "print(f\"   Model size: {sys.getsizeof(ridge_model)} bytes\")\n",
    "\n",
    "# 1. Single Prediction Latency (Most Important for BNPL)\n",
    "print(f\"\\n1️⃣ Single Prediction Latency Test:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Prepare single sample for prediction\n",
    "single_sample = X_test.iloc[[0]]  # First test sample\n",
    "single_sample_scaled = scaler.transform(single_sample)\n",
    "\n",
    "# Warm up (JIT compilation, cache loading)\n",
    "for _ in range(10):\n",
    "    _ = ridge_model.decision_function(single_sample_scaled)\n",
    "\n",
    "# Time single predictions\n",
    "single_times = []\n",
    "n_single_tests = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(n_single_tests):\n",
    "    pred_start = time.perf_counter()\n",
    "    prediction = ridge_model.decision_function(single_sample_scaled)\n",
    "    pred_end = time.perf_counter()\n",
    "    single_times.append((pred_end - pred_start) * 1000)  # Convert to ms\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Statistics\n",
    "single_mean = np.mean(single_times)\n",
    "single_median = np.median(single_times)\n",
    "single_p95 = np.percentile(single_times, 95)\n",
    "single_p99 = np.percentile(single_times, 99)\n",
    "single_max = np.max(single_times)\n",
    "\n",
    "print(f\"Single prediction latency (1000 tests):\")\n",
    "print(f\"   Mean: {single_mean:.3f} ms\")\n",
    "print(f\"   Median: {single_median:.3f} ms\")\n",
    "print(f\"   95th percentile: {single_p95:.3f} ms\")\n",
    "print(f\"   99th percentile: {single_p99:.3f} ms\")\n",
    "print(f\"   Max: {single_max:.3f} ms\")\n",
    "print(f\"   Target: <100 ms → {'✅ PASS' if single_p99 < 100 else '❌ FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Batch Prediction Latency (For high volume scenarios)\n",
    "print(f\"\\n2️⃣ Batch Prediction Latency Test:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "batch_sizes = [1, 10, 100, 1000, 5000]\n",
    "batch_results = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if batch_size <= len(X_test):\n",
    "        batch_sample = X_test.iloc[:batch_size]\n",
    "        batch_sample_scaled = scaler.transform(batch_sample)\n",
    "\n",
    "        # Warm up\n",
    "        for _ in range(5):\n",
    "            _ = ridge_model.decision_function(batch_sample_scaled)\n",
    "\n",
    "        # Time batch predictions\n",
    "        batch_times = []\n",
    "        n_batch_tests = max(10, 100 // batch_size)  # Fewer tests for larger batches\n",
    "\n",
    "        for _ in range(n_batch_tests):\n",
    "            start = time.perf_counter()\n",
    "            predictions = ridge_model.decision_function(batch_sample_scaled)\n",
    "            end = time.perf_counter()\n",
    "            batch_times.append(end - start)\n",
    "\n",
    "        avg_batch_time = np.mean(batch_times)\n",
    "        per_sample_time = (avg_batch_time / batch_size) * 1000  # ms per sample\n",
    "\n",
    "        batch_results[batch_size] = {\n",
    "            'total_time_ms': avg_batch_time * 1000,\n",
    "            'per_sample_ms': per_sample_time,\n",
    "            'throughput_per_sec': batch_size / avg_batch_time\n",
    "        }\n",
    "\n",
    "        print(f\"Batch size {batch_size:4d}: {per_sample_time:.3f} ms/sample, \"\n",
    "            f\"{batch_results[batch_size]['throughput_per_sec']:8.0f} predictions/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocessing Overhead Analysis\n",
    "print(f\"\\n3️⃣ Preprocessing Overhead Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Time just preprocessing\n",
    "preprocess_times = []\n",
    "for _ in range(100):\n",
    "    start = time.perf_counter()\n",
    "    scaled_sample = scaler.transform(single_sample)\n",
    "    end = time.perf_counter()\n",
    "    preprocess_times.append((end - start) * 1000)\n",
    "\n",
    "preprocess_mean = np.mean(preprocess_times)\n",
    "preprocess_p95 = np.percentile(preprocess_times, 95)\n",
    "\n",
    "# Time just model prediction (on pre-scaled data)\n",
    "model_times = []\n",
    "for _ in range(100):\n",
    "    start = time.perf_counter()\n",
    "    prediction = ridge_model.decision_function(single_sample_scaled)\n",
    "    end = time.perf_counter()\n",
    "    model_times.append((end - start) * 1000)\n",
    "\n",
    "model_mean = np.mean(model_times)\n",
    "model_p95 = np.percentile(model_times, 95)\n",
    "\n",
    "print(f\"Preprocessing time: {preprocess_mean:.3f} ms (95th: {preprocess_p95:.3f} ms)\")\n",
    "print(f\"Model inference time: {model_mean:.3f} ms (95th: {model_p95:.3f} ms)\")\n",
    "print(f\"Total pipeline time: {preprocess_mean + model_mean:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Memory Usage Analysis\n",
    "print(f\"\\n4️⃣ Memory Usage Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "# Model size\n",
    "model_size = sys.getsizeof(pickle.dumps(ridge_model))\n",
    "scaler_size = sys.getsizeof(pickle.dumps(scaler))\n",
    "total_size = model_size + scaler_size\n",
    "\n",
    "print(f\"Ridge model size: {model_size:,} bytes ({model_size/1024:.1f} KB)\")\n",
    "print(f\"Scaler size: {scaler_size:,} bytes ({scaler_size/1024:.1f} KB)\")\n",
    "print(f\"Total pipeline size: {total_size:,} bytes ({total_size/1024:.1f} KB)\")\n",
    "print(f\"Memory efficiency: {'✅ Excellent' if total_size < 1024*1024 else '⚠️ Large'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Production Readiness Assessment\n",
    "print(f\"\\n5️⃣ Production Readiness Assessment:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Requirements check\n",
    "requirements = {\n",
    "    'Inference Latency': {\n",
    "        'requirement': '<100ms',\n",
    "        'actual': f'{single_p99:.1f}ms (99th percentile)',\n",
    "        'status': '✅ PASS' if single_p99 < 100 else '❌ FAIL'\n",
    "    },\n",
    "    'Model Size': {\n",
    "        'requirement': '<50MB',\n",
    "        'actual': f'{total_size/1024/1024:.1f}MB',\n",
    "        'status': '✅ PASS' if total_size < 50*1024*1024 else '❌ FAIL'\n",
    "    },\n",
    "    'Throughput': {\n",
    "        'requirement': '100K+ predictions/min',\n",
    "        'actual': f'{batch_results[1000][\"throughput_per_sec\"]*60:,.0f} predictions/min',\n",
    "        'status': '✅ PASS' if batch_results[1000][\"throughput_per_sec\"]*60 > 100000 else '❌ FAIL'\n",
    "    }\n",
    "}\n",
    "\n",
    "for metric, details in requirements.items():\n",
    "    print(f\"{metric:15} | {details['requirement']:12} | {details['actual']:20} | {details['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Latency Distribution Visualization\n",
    "print(f\"\\n6️⃣ Latency Distribution Visualization:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Single prediction latency histogram\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(single_times, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(single_mean, color='red', linestyle='--', label=f'Mean: {single_mean:.2f}ms')\n",
    "plt.axvline(single_p95, color='orange', linestyle='--', label=f'95th: {single_p95:.2f}ms')\n",
    "plt.axvline(100, color='green', linestyle='-', linewidth=2, label='100ms Requirement')\n",
    "plt.xlabel('Latency (ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Single Prediction Latency Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Batch size vs latency per sample\n",
    "plt.subplot(2, 2, 2)\n",
    "batch_sizes_plot = list(batch_results.keys())\n",
    "per_sample_times = [batch_results[bs]['per_sample_ms'] for bs in batch_sizes_plot]\n",
    "plt.plot(batch_sizes_plot, per_sample_times, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Latency per Sample (ms)')\n",
    "plt.title('Batch Size vs Per-Sample Latency')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Throughput vs batch size\n",
    "plt.subplot(2, 2, 3)\n",
    "throughputs = [batch_results[bs]['throughput_per_sec'] for bs in batch_sizes_plot]\n",
    "plt.plot(batch_sizes_plot, throughputs, 'go-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Throughput (predictions/sec)')\n",
    "plt.title('Throughput vs Batch Size')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Component breakdown\n",
    "plt.subplot(2, 2, 4)\n",
    "components = ['Preprocessing', 'Model Inference']\n",
    "times = [preprocess_mean, model_mean]\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "plt.bar(components, times, color=colors, edgecolor='black')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.title('Pipeline Component Breakdown')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on bars\n",
    "for i, (comp, time_val) in enumerate(zip(components, times)):\n",
    "    plt.text(i, time_val + 0.001, f'{time_val:.3f}ms', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Production Deployment Summary\n",
    "print(f\"\\n7️⃣ Production Deployment Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if single_p99 < 100 and total_size < 50*1024*1024:\n",
    "    print(\"🎉 Ridge Model APPROVED for Production Deployment!\")\n",
    "    print(f\"   ✅ Meets all latency requirements ({single_p99:.1f}ms < 100ms)\")\n",
    "    print(f\"   ✅ Lightweight model ({total_size/1024:.1f}KB << 50MB)\")\n",
    "    print(f\"   ✅ High throughput capability ({batch_results[1000]['throughput_per_sec']:,.0f} pred/sec)\")\n",
    "    print(f\"   ✅ Minimal infrastructure requirements\")\n",
    "else:\n",
    "    print(\"⚠️ Ridge Model needs optimization for production:\")\n",
    "    if single_p99 >= 100:\n",
    "        print(f\"   ❌ Latency too high: {single_p99:.1f}ms\")\n",
    "    if total_size >= 50*1024*1024:\n",
    "        print(f\"   ❌ Model too large: {total_size/1024/1024:.1f}MB\")\n",
    "\n",
    "# Save benchmarking results\n",
    "benchmark_results = {\n",
    "    'model_type': 'RidgeClassifier',\n",
    "    'single_prediction_latency_ms': {\n",
    "        'mean': single_mean,\n",
    "        'p95': single_p95,\n",
    "        'p99': single_p99,\n",
    "        'max': single_max\n",
    "    },\n",
    "    'batch_performance': batch_results,\n",
    "    'preprocessing_overhead_ms': preprocess_mean,\n",
    "    'model_inference_ms': model_mean,\n",
    "    'model_size_bytes': total_size,\n",
    "    'production_ready': single_p99 < 100 and total_size < 50*1024*1024\n",
    "}\n",
    "\n",
    "print(f\"\\n💾 Benchmark results saved for production planning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis for Ridge Model\n",
    "\n",
    "# Get our final Ridge model and feature names\n",
    "ridge_model = tuned_models['Ridge']['model']\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "print(f\"🎯 Model: Ridge Classifier\")\n",
    "print(f\"   Features analyzed: {len(feature_names)}\")\n",
    "print(f\"   Model parameters: {ridge_model.get_params()}\")\n",
    "\n",
    "# 1. Linear Coefficients Analysis (Ridge-specific)\n",
    "print(f\"\\n1️⃣ Linear Coefficients Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = ridge_model.coef_[0]  # Ridge is binary classifier\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': np.abs(coefficients),\n",
    "    'importance_rank': range(1, len(feature_names) + 1)\n",
    "}).sort_values('abs_coefficient', ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_importance_df['importance_rank'] = range(1, len(feature_importance_df) + 1)\n",
    "\n",
    "print(f\"Top 10 Most Important Features (by absolute coefficient):\")\n",
    "print(feature_importance_df.head(10)[['feature', 'coefficient', 'abs_coefficient']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nCoefficient Statistics:\")\n",
    "print(f\"   Range: [{coefficients.min():.4f}, {coefficients.max():.4f}]\")\n",
    "print(f\"   Mean absolute: {np.abs(coefficients).mean():.4f}\")\n",
    "print(f\"   Std: {coefficients.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Permutation Importance (Model-agnostic)\n",
    "print(f\"\\n2️⃣ Permutation Importance Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate permutation importance on test set\n",
    "print(\"Calculating permutation importance (this may take a moment)...\")\n",
    "perm_importance = permutation_importance(\n",
    "    ridge_model, X_test_processed, y_test,\n",
    "    n_repeats=10, random_state=42,\n",
    "    scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create permutation importance dataframe\n",
    "perm_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'perm_importance': perm_importance.importances_mean,\n",
    "    'perm_std': perm_importance.importances_std\n",
    "}).sort_values('perm_importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Top 10 Features by Permutation Importance:\")\n",
    "print(perm_df.head(10)[['feature', 'perm_importance', 'perm_std']].to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Categories Analysis\n",
    "print(f\"\\n3️⃣ Feature Categories Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Categorize features\n",
    "def categorize_feature(feature_name):\n",
    "    if any(x in feature_name for x in ['customer_credit_score', 'customer_age', 'customer_income', 'customer_verification']):\n",
    "        return 'Customer Demographics'\n",
    "    elif any(x in feature_name for x in ['device_type', 'device_is_trusted']):\n",
    "        return 'Device/Technology'\n",
    "    elif any(x in feature_name for x in ['payment_provider', 'payment_credit_limit']):\n",
    "        return 'Payment Context'\n",
    "    elif any(x in feature_name for x in ['product_category', 'product_risk']):\n",
    "        return 'Product Features'\n",
    "    elif any(x in feature_name for x in ['purchase_context', 'price_comparison']):\n",
    "        return 'Purchase Behavior'\n",
    "    elif any(x in feature_name for x in ['risk_score', 'risk_scenario', 'risk_level']):\n",
    "        return 'Risk Assessment'\n",
    "    elif any(x in feature_name for x in ['is_weekend', 'is_month_end', 'is_holiday', 'is_business', 'is_late', 'time_of_day', 'week_of_month']):\n",
    "        return 'Temporal Features'\n",
    "    elif feature_name in ['amount', 'installment_count', 'customer_tenure_days']:\n",
    "        return 'Transaction Details'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Add categories to importance dataframes\n",
    "feature_importance_df['category'] = feature_importance_df['feature'].apply(categorize_feature)\n",
    "perm_df['category'] = perm_df['feature'].apply(categorize_feature)\n",
    "\n",
    "# Category-wise importance\n",
    "category_importance = feature_importance_df.groupby('category').agg({\n",
    "    'abs_coefficient': ['mean', 'sum', 'count'],\n",
    "    'coefficient': ['mean']\n",
    "}).round(4)\n",
    "\n",
    "category_importance.columns = ['avg_abs_coef', 'total_abs_coef', 'feature_count', 'avg_coef']\n",
    "category_importance = category_importance.sort_values('total_abs_coef', ascending=False)\n",
    "\n",
    "print(\"Feature Category Importance Summary:\")\n",
    "print(category_importance.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualization Dashboard\n",
    "print(f\"\\n4️⃣ Feature Importance Visualizations:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Ridge Model Feature Importance Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Top 15 coefficients\n",
    "ax1 = axes[0, 0]\n",
    "top_15 = feature_importance_df.head(15)\n",
    "colors = ['red' if x < 0 else 'blue' for x in top_15['coefficient']]\n",
    "bars = ax1.barh(range(len(top_15)), top_15['coefficient'], color=colors, alpha=0.7)\n",
    "ax1.set_yticks(range(len(top_15)))\n",
    "ax1.set_yticklabels(top_15['feature'], fontsize=8)\n",
    "ax1.set_xlabel('Coefficient Value')\n",
    "ax1.set_title('Top 15 Features by Coefficient')\n",
    "ax1.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Permutation importance\n",
    "ax2 = axes[0, 1]\n",
    "top_15_perm = perm_df.head(15)\n",
    "bars2 = ax2.barh(range(len(top_15_perm)), top_15_perm['perm_importance'],\n",
    "                xerr=top_15_perm['perm_std'], alpha=0.7, color='green')\n",
    "ax2.set_yticks(range(len(top_15_perm)))\n",
    "ax2.set_yticklabels(top_15_perm['feature'], fontsize=8)\n",
    "ax2.set_xlabel('Permutation Importance')\n",
    "ax2.set_title('Top 15 Features by Permutation Importance')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Category-wise importance\n",
    "ax3 = axes[0, 2]\n",
    "category_plot = category_importance.sort_values('total_abs_coef', ascending=True)\n",
    "ax3.barh(range(len(category_plot)), category_plot['total_abs_coef'], alpha=0.7, color='orange')\n",
    "ax3.set_yticks(range(len(category_plot)))\n",
    "ax3.set_yticklabels(category_plot.index, fontsize=10)\n",
    "ax3.set_xlabel('Total Absolute Coefficient')\n",
    "ax3.set_title('Feature Category Importance')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Coefficient distribution\n",
    "ax4 = axes[1, 0]\n",
    "ax4.hist(coefficients, bins=30, alpha=0.7, edgecolor='black', color='purple')\n",
    "ax4.axvline(coefficients.mean(), color='red', linestyle='--', label=f'Mean: {coefficients.mean():.3f}')\n",
    "ax4.axvline(0, color='black', linestyle='-', alpha=0.5)\n",
    "ax4.set_xlabel('Coefficient Value')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Coefficient Distribution')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Coefficient vs Permutation Importance\n",
    "ax5 = axes[1, 1]\n",
    "comparison_df = pd.merge(feature_importance_df[['feature', 'abs_coefficient']],\n",
    "                        perm_df[['feature', 'perm_importance']], on='feature')\n",
    "ax5.scatter(comparison_df['abs_coefficient'], comparison_df['perm_importance'], alpha=0.6, s=50)\n",
    "\n",
    "# Add correlation line\n",
    "from scipy.stats import pearsonr\n",
    "corr, p_value = pearsonr(comparison_df['abs_coefficient'], comparison_df['perm_importance'])\n",
    "ax5.set_xlabel('Absolute Coefficient')\n",
    "ax5.set_ylabel('Permutation Importance')\n",
    "ax5.set_title(f'Coefficient vs Permutation Importance\\n(Correlation: {corr:.3f})')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(comparison_df['abs_coefficient'], comparison_df['perm_importance'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax5.plot(comparison_df['abs_coefficient'], p(comparison_df['abs_coefficient']), \"r--\", alpha=0.8)\n",
    "\n",
    "# Plot 6: Feature importance by category (box plot)\n",
    "ax6 = axes[1, 2]\n",
    "category_data = []\n",
    "category_labels = []\n",
    "for cat in feature_importance_df['category'].unique():\n",
    "    cat_data = feature_importance_df[feature_importance_df['category'] == cat]['abs_coefficient']\n",
    "    category_data.append(cat_data)\n",
    "    category_labels.append(cat)\n",
    "\n",
    "ax6.boxplot(category_data, labels=category_labels)\n",
    "ax6.set_xticklabels(category_labels, rotation=45, ha='right', fontsize=8)\n",
    "ax6.set_ylabel('Absolute Coefficient')\n",
    "ax6.set_title('Feature Importance Distribution by Category')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Business Insights\n",
    "print(f\"\\n5️⃣ Business Insights from Feature Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Most predictive features\n",
    "top_business_features = feature_importance_df.head(10)\n",
    "print(\"🔍 Top Predictive Features for BNPL Default Risk:\")\n",
    "for i, row in top_business_features.iterrows():\n",
    "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"   {i+1}. {row['feature']}: {direction} default risk (coef: {row['coefficient']:.4f})\")\n",
    "\n",
    "# Category insights\n",
    "print(f\"\\n📊 Category-Level Insights:\")\n",
    "for category, data in category_importance.iterrows():\n",
    "    avg_impact = \"High\" if data['avg_abs_coef'] > coefficients.std() else \"Medium\" if data['avg_abs_coef'] > coefficients.std()/2 else \"Low\"\n",
    "    print(f\"   {category}: {avg_impact} impact ({data['feature_count']} features, avg coef: {data['avg_abs_coef']:.4f})\")\n",
    "\n",
    "# Model interpretability summary\n",
    "print(f\"\\n🎯 Model Interpretability Summary:\")\n",
    "print(f\"   Total features: {len(feature_names)}\")\n",
    "print(f\"   Features with |coef| > {coefficients.std():.4f}: {sum(feature_importance_df['abs_coefficient'] > coefficients.std())}\")\n",
    "print(f\"   Most important category: {category_importance.index[0]}\")\n",
    "print(f\"   Feature correlation with perm. importance: {corr:.3f} (p={p_value:.3f})\")\n",
    "\n",
    "# Save feature importance results\n",
    "feature_analysis_results = {\n",
    "    'model_type': 'RidgeClassifier',\n",
    "    'top_features': top_business_features[['feature', 'coefficient', 'abs_coefficient']].to_dict('records'),\n",
    "    'category_importance': category_importance.to_dict('index'),\n",
    "    'coefficient_stats': {\n",
    "        'mean': float(coefficients.mean()),\n",
    "        'std': float(coefficients.std()),\n",
    "        'min': float(coefficients.min()),\n",
    "        'max': float(coefficients.max())\n",
    "    },\n",
    "    'correlation_coef_vs_perm': float(corr)\n",
    "}\n",
    "\n",
    "print(f\"\\n💾 Feature analysis results saved for production documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Model Eval\n",
    "\n",
    "Before exporting model artifacts for production, it became apparent that we were leaving some business usefulness on the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business-Focused Model Evaluation for BNPL Default Risk\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Business cost parameters\n",
    "AVG_TRANSACTION_AMOUNT = 144  \n",
    "AVG_DOWNPAYMENT_RATE = 0.25   # 25% downpayment typical for BNPL\n",
    "CHURN_PROBABILITY = 0.15      # 20% customers abandon purchase if BNPL rejected. Potentially lower because of the AOV, which is >2X Amazon's, pointing to less price sensitivity.\n",
    "\n",
    "# Cost calculations\n",
    "def calculate_business_costs(amount=AVG_TRANSACTION_AMOUNT):\n",
    "    missed_default_cost = amount * (1 - AVG_DOWNPAYMENT_RATE)  # Loss if we lend to defaulter\n",
    "    rejected_good_customer_cost = amount * CHURN_PROBABILITY    # Revenue loss if we reject good customer\n",
    "    return missed_default_cost, rejected_good_customer_cost\n",
    "\n",
    "missed_default_cost, rejected_good_cost = calculate_business_costs()\n",
    "print(f\"📊 Business Cost Structure:\")\n",
    "print(f\"   • Cost of missed default: ${missed_default_cost:.2f}\")\n",
    "print(f\"   • Cost of rejected good customer: ${rejected_good_cost:.2f}\")\n",
    "print(f\"   • Cost ratio (FN:FP): {missed_default_cost/rejected_good_cost:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_business_metrics_comprehensive(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive business evaluation for BNPL models\n",
    "    Handles both probability-based and score-based models\n",
    "    \"\"\"\n",
    "    # Get predictions - handle RidgeClassifier vs LogisticRegression/ElasticNet\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        # LogisticRegression and ElasticNet have predict_proba\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        # RidgeClassifier has decision_function - convert to probabilities\n",
    "        decision_scores = model.decision_function(X_test)\n",
    "        # Convert decision scores to probabilities using sigmoid\n",
    "        from scipy.special import expit\n",
    "        y_pred_proba = expit(decision_scores)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} doesn't support probability predictions\")\n",
    "\n",
    "    # Calculate precision-recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "    # Key metrics\n",
    "    auc_pr = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Business-critical metrics at different precision levels\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'auc_pr': auc_pr,\n",
    "    }\n",
    "\n",
    "    # Find recall at specific precision thresholds\n",
    "    # Note: thresholds array is one element shorter than precision/recall\n",
    "    for target_precision in [0.90, 0.95, 0.98]:\n",
    "        valid_indices = precision[:-1] >= target_precision  # Exclude last element to match thresholds\n",
    "        if valid_indices.any():\n",
    "            valid_recall = recall[:-1][valid_indices]  # Exclude last element\n",
    "            max_recall = valid_recall.max()\n",
    "            optimal_threshold_idx = valid_recall.argmax()\n",
    "            optimal_threshold = thresholds[valid_indices][optimal_threshold_idx]\n",
    "        else:\n",
    "            max_recall = 0.0\n",
    "            optimal_threshold = 1.0\n",
    "\n",
    "        results[f'recall_at_{int(target_precision*100)}_precision'] = max_recall\n",
    "        results[f'threshold_for_{int(target_precision*100)}_precision'] = optimal_threshold\n",
    "\n",
    "    # Find precision at specific recall thresholds\n",
    "    for target_recall in [0.70, 0.80, 0.90]:\n",
    "        valid_indices = recall[:-1] >= target_recall  # Exclude last element to match thresholds\n",
    "        if valid_indices.any():\n",
    "            valid_precision = precision[:-1][valid_indices]  # Exclude last element\n",
    "            max_precision = valid_precision.max()\n",
    "            optimal_threshold_idx = valid_precision.argmax()\n",
    "            optimal_threshold = thresholds[valid_indices][optimal_threshold_idx]\n",
    "        else:\n",
    "            max_precision = 0.0\n",
    "            optimal_threshold = 0.0\n",
    "\n",
    "        results[f'precision_at_{int(target_recall*100)}_recall'] = max_precision\n",
    "        results[f'threshold_for_{int(target_recall*100)}_recall'] = optimal_threshold\n",
    "\n",
    "    # Expected business value calculation\n",
    "    # Find threshold that maximizes business value\n",
    "    best_value = -float('inf')\n",
    "    best_threshold = 0.5\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    for threshold in np.linspace(0.1, 0.9, 50):\n",
    "        y_pred_binary = (y_pred_proba >= threshold).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "            # Business value calculation\n",
    "            # Revenue saved by catching defaults - revenue lost by rejecting good customers\n",
    "            value_saved = tp * missed_default_cost  # True positives: defaults caught\n",
    "            value_lost = fp * rejected_good_cost    # False positives: good customers rejected\n",
    "            net_business_value = value_saved - value_lost\n",
    "\n",
    "            if net_business_value > best_value:\n",
    "                best_value = net_business_value\n",
    "                best_threshold = threshold\n",
    "                best_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                best_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                best_f1 = 2 * (best_precision * best_recall) / (best_precision + best_recall) if (best_precision + best_recall) > 0 else 0\n",
    "\n",
    "    results.update({\n",
    "        'optimal_business_threshold': best_threshold,\n",
    "        'max_business_value': best_value,\n",
    "        'precision_at_optimal_threshold': best_precision,\n",
    "        'recall_at_optimal_threshold': best_recall,\n",
    "        'f1_at_optimal_threshold': best_f1\n",
    "    })\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"✅ Business evaluation function defined (fixed array dimension issue)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models with business metrics\n",
    "models_to_evaluate = {\n",
    "    'Ridge': tuned_models['Ridge']['model'],\n",
    "    'LogisticRegression': tuned_models['LogisticRegression']['model'],\n",
    "    'ElasticNet': tuned_models['ElasticNet']['model']\n",
    "}\n",
    "\n",
    "business_results = {}\n",
    "\n",
    "print(\"🔄 Evaluating models with business metrics...\")\n",
    "print()\n",
    "\n",
    "for model_name, model in models_to_evaluate.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "    # Use scaled data for linear models\n",
    "    X_eval = X_test_processed\n",
    "\n",
    "    results = evaluate_business_metrics_comprehensive(model, X_eval, y_test, model_name)\n",
    "    business_results[model_name] = results\n",
    "\n",
    "    print(f\"✅ {model_name} evaluation complete\")\n",
    "\n",
    "print(f\"\\n📊 Business evaluation complete for {len(models_to_evaluate)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "# Convert results to DataFrame\n",
    "comparison_df = pd.DataFrame(business_results).T\n",
    "\n",
    "# Key business metrics for ranking\n",
    "key_metrics = [\n",
    "    'auc_pr',\n",
    "    'recall_at_95_precision',\n",
    "    'recall_at_90_precision',\n",
    "    'precision_at_80_recall',\n",
    "    'max_business_value',\n",
    "    'optimal_business_threshold',\n",
    "    'f1_at_optimal_threshold'\n",
    "]\n",
    "\n",
    "print(\"🏆 BUSINESS METRICS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display comparison table\n",
    "display_df = comparison_df[key_metrics].round(4)\n",
    "print(display_df.to_string())\n",
    "\n",
    "print(f\"\\n💰 BUSINESS VALUE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "for model_name in business_results.keys():\n",
    "    value = business_results[model_name]['max_business_value']\n",
    "    threshold = business_results[model_name]['optimal_business_threshold']\n",
    "    precision = business_results[model_name]['precision_at_optimal_threshold']\n",
    "    recall = business_results[model_name]['recall_at_optimal_threshold']\n",
    "\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  • Max Business Value: ${value:.2f}\")\n",
    "    print(f\"  • Optimal Threshold: {threshold:.3f}\")\n",
    "    print(f\"  • Precision at Optimal: {precision:.3f}\")\n",
    "    print(f\"  • Recall at Optimal: {recall:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CELL - Add this to investigate the issue\n",
    "print(\"🔍 DIAGNOSTIC ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check basic model performance\n",
    "for model_name, model in models_to_evaluate.items():\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        decision_scores = model.decision_function(X_test_processed)\n",
    "        from scipy.special import expit\n",
    "        y_pred_proba = expit(decision_scores)\n",
    "\n",
    "    y_pred_binary = model.predict(X_test_processed)\n",
    "\n",
    "    # Basic metrics\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  • Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  • AUC-ROC: {auc_roc:.3f}\")\n",
    "    print(f\"  • Prediction range: {y_pred_proba.min():.3f} - {y_pred_proba.max():.3f}\")\n",
    "    print(f\"  • Mean prediction: {y_pred_proba.mean():.3f}\")\n",
    "    print(f\"  • Predictions > 0.5: {(y_pred_proba > 0.5).sum()}\")\n",
    "\n",
    "# Check test set composition\n",
    "print(f\"\\n📊 Test Set Analysis:\")\n",
    "print(f\"  • Total samples: {len(y_test)}\")\n",
    "print(f\"  • Actual defaults: {y_test.sum()}\")\n",
    "print(f\"  • Default rate: {y_test.mean():.3f}\")\n",
    "\n",
    "# Check if there's a class imbalance issue\n",
    "print(f\"\\n⚠️  POTENTIAL ISSUES:\")\n",
    "if y_test.mean() < 0.02:\n",
    "    print(\"  • Extremely low default rate - models may default to 'no default'\")\n",
    "if all([(y_pred_proba.max() < 0.5) for model_name, model in models_to_evaluate.items() for y_pred_proba in [model.predict_proba(X_test_processed)[:, 1] if hasattr(model, 'predict_proba') else\n",
    "expit(model.decision_function(X_test_processed))]]):\n",
    "    print(\"  • All models predict very low probabilities\")\n",
    "\n",
    "# Check feature scaling impact\n",
    "print(f\"\\n🔧 Feature Analysis:\")\n",
    "print(f\"  • X_test_processed shape: {X_test_processed.shape}\")\n",
    "print(f\"  • X_test_processed range: {X_test_processed.min():.3f} - {X_test_processed.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance at appropriate thresholds for imbalanced data\n",
    "print(\"🎯 IMBALANCED DATA APPROPRIATE THRESHOLDS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, model in models_to_evaluate.items():\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        from scipy.special import expit\n",
    "        y_pred_proba = expit(model.decision_function(X_test_processed))\n",
    "\n",
    "    # Use percentile-based thresholds instead of absolute thresholds\n",
    "    thresholds = [np.percentile(y_pred_proba, p) for p in [90, 95, 98, 99]]\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for i, threshold in enumerate([90, 95, 98, 99]):\n",
    "        thresh_val = thresholds[i]\n",
    "        y_pred_binary = (y_pred_proba >= thresh_val).astype(int)\n",
    "\n",
    "        if y_pred_binary.sum() > 0:  # Avoid division by zero\n",
    "            precision = (y_pred_binary & y_test).sum() / y_pred_binary.sum()\n",
    "            recall = (y_pred_binary & y_test).sum() / y_test.sum()\n",
    "            print(f\"  • Top {100-threshold}%: Precision={precision:.3f}, Recall={recall:.3f}, Threshold={thresh_val:.3f}\")\n",
    "        else:\n",
    "            print(f\"  • Top {100-threshold}%: No predictions above threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business-focused ranking with weighted criteria\n",
    "print(\"🎯 BUSINESS-FOCUSED MODEL RANKING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define business priorities (weights sum to 1.0)\n",
    "ranking_weights = {\n",
    "    'recall_at_95_precision': 0.35,    # Primary: catch defaults conservatively  \n",
    "    'auc_pr': 0.25,                    # Overall performance for imbalanced data\n",
    "    'max_business_value': 0.25,        # Economic impact\n",
    "    'recall_at_90_precision': 0.15,    # Secondary: less conservative recall\n",
    "}\n",
    "\n",
    "print(\"📋 Ranking Criteria:\")\n",
    "for metric, weight in ranking_weights.items():\n",
    "    print(f\"   • {metric}: {weight*100:.1f}%\")\n",
    "\n",
    "# Calculate weighted scores\n",
    "model_scores = {}\n",
    "\n",
    "for model_name in business_results.keys():\n",
    "    score = 0\n",
    "    for metric, weight in ranking_weights.items():\n",
    "        metric_value = business_results[model_name][metric]\n",
    "        # Normalize to 0-1 scale based on max value across models\n",
    "        max_value = max([business_results[m][metric] for m in business_results.keys()])\n",
    "        if max_value > 0:\n",
    "            normalized_value = metric_value / max_value\n",
    "        else:\n",
    "            normalized_value = 0\n",
    "        score += weight * normalized_value\n",
    "\n",
    "    model_scores[model_name] = score\n",
    "\n",
    "# Rank models\n",
    "ranked_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n🏅 BUSINESS-FOCUSED RANKINGS:\")\n",
    "print(\"-\" * 30)\n",
    "for i, (model_name, score) in enumerate(ranked_models, 1):\n",
    "    auc_pr = business_results[model_name]['auc_pr']\n",
    "    recall_95 = business_results[model_name]['recall_at_95_precision']\n",
    "    business_value = business_results[model_name]['max_business_value']\n",
    "\n",
    "    status = \"🥇 CHAMPION\" if i == 1 else f\"🥈 CHALLENGER #{i-1}\"\n",
    "\n",
    "    print(f\"{i}. {model_name} - Score: {score:.3f} {status}\")\n",
    "    print(f\"   • AUC-PR: {auc_pr:.3f}\")\n",
    "    print(f\"   • Recall@95%Prec: {recall_95:.3f}\")\n",
    "    print(f\"   • Business Value: ${business_value:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Update champion selection\n",
    "business_champion = ranked_models[0][0]\n",
    "print(f\"🎯 BUSINESS CHAMPION: {business_champion}\")\n",
    "print(f\"   (Previous AUC-ROC champion: Ridge)\")\n",
    "\n",
    "if business_champion != 'Ridge':\n",
    "    print(f\"⚠️  CHAMPION CHANGE DETECTED!\")\n",
    "    print(f\"   Business metrics favor {business_champion} over Ridge\")\n",
    "else:\n",
    "    print(f\"✅ Ridge confirmed as champion by business metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create business metrics visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Precision-Recall Curves\n",
    "ax1 = axes[0, 0]\n",
    "for model_name, model in models_to_evaluate.items():\n",
    "# Handle different model types\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        decision_scores = model.decision_function(X_test_processed)\n",
    "        from scipy.special import expit\n",
    "        y_pred_proba = expit(decision_scores)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auc_pr = business_results[model_name]['auc_pr']\n",
    "    ax1.plot(recall, precision, label=f'{model_name} (AUC-PR: {auc_pr:.3f})')\n",
    "\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title('Precision-Recall Curves')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Business Value Comparison\n",
    "ax2 = axes[0, 1]\n",
    "models = list(business_results.keys())\n",
    "values = [business_results[m]['max_business_value'] for m in models]\n",
    "colors = ['gold' if m == business_champion else 'lightblue' for m in models]\n",
    "\n",
    "bars = ax2.bar(models, values, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Max Business Value ($)')\n",
    "ax2.set_title('Business Value by Model')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Annotate bars\n",
    "for bar, value in zip(bars, values):\n",
    "    ax2.annotate(f'${value:.0f}',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Recall at Different Precision Thresholds\n",
    "ax3 = axes[1, 0]\n",
    "precision_levels = [90, 95, 98]\n",
    "x = np.arange(len(precision_levels))\n",
    "width = 0.25\n",
    "\n",
    "for i, model_name in enumerate(models):\n",
    "    recalls = [business_results[model_name][f'recall_at_{p}_precision'] for p in precision_levels]\n",
    "    ax3.bar(x + i * width, recalls, width, label=model_name, alpha=0.7)\n",
    "\n",
    "ax3.set_xlabel('Precision Threshold (%)')\n",
    "ax3.set_ylabel('Recall')\n",
    "ax3.set_title('Recall at High Precision Thresholds')\n",
    "ax3.set_xticks(x + width)\n",
    "ax3.set_xticklabels([f'{p}%' for p in precision_levels])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Model Ranking Scores\n",
    "ax4 = axes[1, 1]\n",
    "models_ranked = [item[0] for item in ranked_models]\n",
    "scores = [item[1] for item in ranked_models]\n",
    "colors = ['gold', 'silver', 'chocolate'][:len(models_ranked)]\n",
    "\n",
    "bars = ax4.bar(models_ranked, scores, color=colors, alpha=0.7)\n",
    "ax4.set_ylabel('Composite Business Score')\n",
    "ax4.set_title('Business-Focused Model Ranking')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Annotate champion\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax4.annotate(f'{score:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('BNPL Model Business Performance Dashboard', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Business metrics visualization complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flit-ml-Ja6BlcxJ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
