{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNPL ML Model Development\n",
    "\n",
    "**Objective**: Develop production-ready ML model for BNPL default risk prediction\n",
    "\n",
    "**Performance Targets**:\n",
    "- Beat current 3.5x risk discrimination baseline\n",
    "- Achieve >40% precision on high-risk segment\n",
    "- Maintain <100ms inference latency\n",
    "- Production deployment ready\n",
    "\n",
    "**Focus**: Primary target is `will_default` prediction (binary classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Steps\n",
    "\n",
    "### **Step 1: Algorithm Research & Selection** \n",
    "- Evaluate ML algorithms for BNPL risk assessment\n",
    "- Consider inference latency requirements (<100ms)\n",
    "- Assess interpretability for regulatory compliance\n",
    "- Select candidate algorithms for testing\n",
    "\n",
    "### **Step 2: Data Loading & Feature Engineering**\n",
    "- Load engineered features using `BNPLFeatureEngineer` class\n",
    "- Validate feature quality and distribution\n",
    "- Prepare data for modeling\n",
    "\n",
    "### **Step 3: Baseline Performance Establishment**\n",
    "- Implement current underwriting baseline (3.5x discrimination)\n",
    "- Train candidate models on engineered features\n",
    "- Establish performance benchmarks\n",
    "\n",
    "### **Step 4: Model Training & Evaluation**\n",
    "- Cross-validation framework\n",
    "- Business metrics: discrimination ratio, precision/recall\n",
    "- Technical metrics: latency, model size, memory usage\n",
    "- Feature importance analysis\n",
    "\n",
    "### **Step 5: Production Readiness Assessment**\n",
    "- Inference latency benchmarking\n",
    "- Model serialization and deployment format\n",
    "- Edge case handling and fallback strategies\n",
    "- A/B testing framework preparation\n",
    "\n",
    "### **Step 6: Model Selection & Recommendations**\n",
    "- Compare algorithms across business and technical dimensions\n",
    "- Select final model for production deployment\n",
    "- Document trade-offs and deployment considerations\n",
    "- Prepare ML engineering handoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path for imports\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "print(f\"Project root added to path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom feature engineering class\n",
    "from flit_ml.features.bnpl_feature_engineering import BNPLFeatureEngineer\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"ML development environment ready!\")\n",
    "print(f\"Available ML libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Algorithm Research & Selection\n",
    "\n",
    "### ML Algorithm Comparison for BNPL Risk Assessment\n",
    "\n",
    "| Algorithm | Inference Speed | Interpretability | Performance | Memory Usage | Production Ready | Pros | Cons |\n",
    "|-----------|----------------|------------------|-------------|--------------|------------------|------|------|\n",
    "| **LogisticRegression** | Fastest (~1ms) | High (coefficients) | Good baseline | Minimal | Excellent | Fast inference, Interpretable, Stable, Low memory | Linear assumptions, May miss complex patterns |\n",
    "| **RandomForestClassifier** | Fast (~5-10ms) | Medium (feature importance) | Strong for tabular | Moderate | Good | Handles non-linearity, Feature importance, Robust | Larger model size, Less interpretable |\n",
    "| **XGBoost** | Fast (~10-20ms) | Medium (SHAP values) | Often best for tabular | Moderate | Good | High performance, Feature importance, Handles missing values | Hyperparameter tuning, Model complexity |\n",
    "| **LightGBM** | Very Fast (~5ms) | Medium (SHAP values) | Excellent for tabular | Low | Excellent | Fast training/inference, Memory efficient, High performance | Can overfit small datasets, Less interpretable than linear |\n",
    "\n",
    "### Selection Criteria for BNPL Production System\n",
    "\n",
    "1. **Inference latency**: <100ms (preferably <20ms)\n",
    "2. **Performance**: Beat 3.5x discrimination ratio\n",
    "3. **Interpretability**: Regulatory compliance requirements\n",
    "4. **Production stability**: Minimal maintenance overhead\n",
    "5. **Memory efficiency**: Scalable serving architecture\n",
    "\n",
    "### Recommended Starting Models\n",
    "\n",
    "Based on BNPL constraints, we'll focus on all 4 models before considering more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading & Feature Engineering\n",
    "\n",
    "Load engineered features using our production-ready `BNPLFeatureEngineer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer with clean output for development                                                                                                                                       \n",
    "feature_engineer = BNPLFeatureEngineer(verbose=True)                                                                                                                                               \n",
    "                                                                                                                                                                                                        \n",
    "# Load and engineer features                                                                                                                                                                          \n",
    "print(\"ðŸš€ Loading and engineering features for model development...\")                                                                                                                                 \n",
    "df_features, feature_metadata = feature_engineer.engineer_features(                                                                                                                                   \n",
    "    sample_size=1000,  # Sample size for development                                                                                                                                                  \n",
    "    random_seed=42                                                                                                                                                                                    \n",
    ")                                                                                                                                                                                                     \n",
    "                                                                                                                                                                                                      \n",
    "print(f\"\\nðŸ“Š Feature Engineering Complete:\")                                                                                                                                                          \n",
    "print(f\"   Dataset shape: {df_features.shape}\")                                                                                                                                                       \n",
    "print(f\"   Features available: {len(feature_metadata['all_features'])}\")                                                                                                                              \n",
    "print(f\"   Target variable: {feature_metadata['target_variable']}\")                                                                                                                                   \n",
    "print(f\"   Default rate: {df_features['will_default'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate feature quality and distribution\n",
    "print(\"ðŸ” Feature Quality Assessment:\")\n",
    "print(f\"   Data shape: {df_features.shape}\")\n",
    "print(f\"   Memory usage: {df_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"   Default rate: {df_features['will_default'].mean():.1%}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "missing_values = df_features.isnull().sum().sum()\n",
    "print(f\"   Missing values: {missing_values}\")\n",
    "\n",
    "# Feature type summary\n",
    "print(f\"\\nðŸ“‹ Feature Types:\")\n",
    "for feature_type, features in feature_metadata.items():\n",
    "    if isinstance(features, list) and feature_type.endswith('_features'):\n",
    "        print(f\"   {feature_type}: {len(features)} features\")\n",
    "\n",
    "# Display sample of features\n",
    "print(f\"\\nðŸ“‹ Sample of engineered features:\")\n",
    "display_cols = df_features.columns[:10].tolist()\n",
    "if 'will_default' not in display_cols:\n",
    "    display_cols.append('will_default')\n",
    "    \n",
    "df_features[display_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"ðŸ”§ Preparing data for model training...\")\n",
    "\n",
    "# Separate features and target - exclude both target variables\n",
    "exclude_cols = ['will_default', 'days_to_first_missed_payment']  # Remove both target variables, even though the 2nd one is secondary (for more enhanced models)\n",
    "feature_cols = [col for col in df_features.columns if col not in exclude_cols]\n",
    "numeric_features = ['amount', 'risk_score', 'payment_credit_limit', 'price_comparison_time', 'customer_tenure_days']\n",
    "categorical_features = [col for col in feature_cols if col not in numeric_features]\n",
    "primary_target_col = exclude_cols[0]  # 'will_default'\n",
    "secondary_target_col = exclude_cols[1]  # 'days_to_first_missed_payment'\n",
    "\n",
    "# For now, we will focus on the primary target\n",
    "target_col = primary_target_col\n",
    "\n",
    "X = df_features[feature_cols]\n",
    "y = df_features[target_col]\n",
    "\n",
    "print(f\"   Features shape: {X.shape}\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "print(f\"   Target distribution: {y.value_counts().to_dict()}\")\n",
    "print(f\"   Class balance: {y.mean():.1%} positive class\")\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Train-Test Split:\")\n",
    "print(f\"   Training set: {X_train.shape}\")\n",
    "print(f\"   Test set: {X_test.shape}\")\n",
    "print(f\"   Train default rate: {y_train.mean():.1%}\")\n",
    "print(f\"   Test default rate: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling for algorithms that need it\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    " # Create preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_features),      # Scale numeric features\n",
    "    ('cat', 'passthrough', categorical_features)      # Leave categorical unchanged\n",
    "])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)  # Fit AND transform train\n",
    "X_test_processed = preprocessor.transform(X_test)        # Only transform test (using train's fit) --> Avoid inconsistent scaling train vs test\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… Data preparation complete\")\n",
    "print(f\"   Ready for model training and evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Baseline Performance Establishment\n",
    "\n",
    "Implement current underwriting baseline and train candidate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current underwriting baseline analysis\n",
    "print(\"ðŸ“Š Current Underwriting Baseline Analysis\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Analyze current risk_score and risk_level performance\n",
    "if 'risk_score' in df_features.columns and 'risk_level_encoded' in df_features.columns:\n",
    "    \n",
    "    # Risk level performance\n",
    "    risk_performance = df_features.groupby('risk_level_encoded')['will_default'].agg(['count', 'mean']).round(3)\n",
    "    risk_performance.columns = ['transaction_count', 'default_rate']\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Current Risk Level Performance:\")\n",
    "    risk_level_mapping = {0: 'Low', 1: 'Medium', 2: 'High'}\n",
    "    for idx, row in risk_performance.iterrows():\n",
    "        level_name = risk_level_mapping.get(idx, f'Level_{idx}')\n",
    "        print(f\"   {level_name} Risk: {row['default_rate']:.1%} default rate ({row['transaction_count']:,} transactions)\")\n",
    "    \n",
    "    # Calculate discrimination ratio\n",
    "    high_risk_rate = risk_performance.loc[2, 'default_rate']  # High risk (encoded as 2)\n",
    "    low_risk_rate = risk_performance.loc[0, 'default_rate']   # Low risk (encoded as 0)\n",
    "    current_discrimination = high_risk_rate / low_risk_rate if low_risk_rate > 0 else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Current System Discrimination:\")\n",
    "    print(f\"   High risk default rate: {high_risk_rate:.1%}\")\n",
    "    print(f\"   Low risk default rate: {low_risk_rate:.1%}\")\n",
    "    print(f\"   Discrimination ratio: {current_discrimination:.1f}x\")\n",
    "    print(f\"   Target to beat: >{current_discrimination:.1f}x\")\n",
    "    \n",
    "    # Risk score distribution analysis\n",
    "    print(f\"\\nðŸ“Š Risk Score Distribution:\")\n",
    "    risk_score_stats = df_features['risk_score'].describe()\n",
    "    print(f\"   Range: {risk_score_stats['min']:.2f} - {risk_score_stats['max']:.2f}\")\n",
    "    print(f\"   Mean: {risk_score_stats['mean']:.2f}\")\n",
    "    print(f\"   Std: {risk_score_stats['std']:.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Risk score/level features not available in dataset\")\n",
    "    current_discrimination = 3.5  # Use known baseline from context\n",
    "    print(f\"   Using known baseline discrimination ratio: {current_discrimination}x\")\n",
    "\n",
    "# Set performance targets\n",
    "target_discrimination = max(current_discrimination * 1.1, 4.0)  # At least 10% improvement or 4.0x\n",
    "target_precision = 0.40  # 40% precision on high-risk segment\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Performance Targets for ML Models:\")\n",
    "print(f\"   Discrimination ratio: >{target_discrimination:.1f}x\")\n",
    "print(f\"   High-risk precision: >{target_precision:.0%}\")\n",
    "print(f\"   Inference latency: <100ms\")\n",
    "print(f\"   Model size: <50MB (for fast loading)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âš ï¸ IMPLEMENTATION GATE\n",
    "\n",
    "**NEXT**: Ready to proceed with Step 4 - Model Training & Evaluation\n",
    "\n",
    "**Current Status**:\n",
    "- âœ… Features engineered and validated\n",
    "- âœ… Algorithms researched and selected\n",
    "- âœ… Baseline performance established\n",
    "- âœ… Data prepared for training\n",
    "\n",
    "**Ready to implement**:\n",
    "- Model training with cross-validation\n",
    "- Business metrics evaluation\n",
    "- Latency benchmarking\n",
    "- Feature importance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flit-ml-Ja6BlcxJ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
